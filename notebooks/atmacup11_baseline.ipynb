{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"atmacup11_baseline_gotaku.ipynb","provenance":[{"file_id":"133HEmBZjLaJL43cgeaQZt8O22NMNLZlk","timestamp":1626157089373}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1608208d63bc412681ba66f593440c57":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4cd7dcc1a0d14dac9e80b280929b7103","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_67aabadae20b45b3bdbfb270fe9c77cf","IPY_MODEL_a21ac66472e14d83832a86736e5b6c1b","IPY_MODEL_cff12f5892f2454d85a91c44e6ec7858"]}},"4cd7dcc1a0d14dac9e80b280929b7103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67aabadae20b45b3bdbfb270fe9c77cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b55fc452e85346519ed3d456766e479e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5af03550542d47609394a81549809e68"}},"a21ac66472e14d83832a86736e5b6c1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b44ef4ec6e14ae484960bffddfbe080","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":185,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":185,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f414cf6a39d49f08872d4db4de03352"}},"cff12f5892f2454d85a91c44e6ec7858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0c02e4ab5db417a96b378de86743005","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 185/185 [00:55&lt;00:00,  3.86it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f4f8bccae2343ba928685ec72339f01"}},"b55fc452e85346519ed3d456766e479e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5af03550542d47609394a81549809e68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b44ef4ec6e14ae484960bffddfbe080":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1f414cf6a39d49f08872d4db4de03352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0c02e4ab5db417a96b378de86743005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f4f8bccae2343ba928685ec72339f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPBSeYZKlAdm","executionInfo":{"status":"ok","timestamp":1626143918908,"user_tz":-540,"elapsed":266,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"6dbe7cc8-878d-4ee5-a725-6d259d1092e3"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Jul 13 02:38:36 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   65C    P8    15W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzBbymkxlmoU","executionInfo":{"status":"ok","timestamp":1626143919824,"user_tz":-540,"elapsed":919,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"8139377c-03bd-4ba2-c491-f9f5943985b6"},"source":["!pip uninstall albumentations -y\n","# !pip uninstall scikit-learn -y"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling albumentations-0.1.12:\n","  Successfully uninstalled albumentations-0.1.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TxUQcieGloTZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626143944696,"user_tz":-540,"elapsed":24874,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"5544e6b9-de6a-4588-fece-115e8dc07dbb"},"source":["!pip install timm -q\n","!pip install albumentations -q\n","!pip install lightly -q\n","# !pip install scikit-learn==0.24.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 378kB 11.7MB/s \n","\u001b[K     |████████████████████████████████| 102kB 6.2MB/s \n","\u001b[K     |████████████████████████████████| 37.2MB 85kB/s \n","\u001b[K     |████████████████████████████████| 245kB 7.8MB/s \n","\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n","\u001b[K     |████████████████████████████████| 153kB 13.6MB/s \n","\u001b[K     |████████████████████████████████| 819kB 13.4MB/s \n","\u001b[K     |████████████████████████████████| 112kB 30.1MB/s \n","\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n","\u001b[K     |████████████████████████████████| 829kB 26.4MB/s \n","\u001b[K     |████████████████████████████████| 10.6MB 12.0MB/s \n","\u001b[K     |████████████████████████████████| 645kB 45.7MB/s \n","\u001b[K     |████████████████████████████████| 235kB 43.5MB/s \n","\u001b[K     |████████████████████████████████| 122kB 60.7MB/s \n","\u001b[K     |████████████████████████████████| 1.3MB 48.4MB/s \n","\u001b[K     |████████████████████████████████| 296kB 48.6MB/s \n","\u001b[K     |████████████████████████████████| 143kB 58.7MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yKbBIlob1VS_"},"source":["import os\n","if not os.path.exists('/content/input/dataset_atmaCup11/'):\n","    os.makedirs('/content/input/dataset_atmaCup11/')\n","!cp /content/drive/MyDrive/atmacup/atmacup11/input/dataset_atmaCup11/* /content/input/dataset_atmaCup11/\n","if not os.path.exists('/content/input/dataset_atmaCup11/photos/'):\n","    !unzip /content/input/dataset_atmaCup11/photos.zip -d /content/input/dataset_atmaCup11/photos/ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aewTUeL8lqza"},"source":["import os\n","import math\n","import time\n","import random\n","import shutil\n","from pathlib import Path\n","from contextlib import contextmanager\n","from collections import defaultdict, Counter\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn import preprocessing\n","from sklearn.metrics import roc_auc_score, mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","from tqdm.auto import tqdm\n","from functools import partial\n","\n","import pickle\n","\n","import cv2\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW, Optimizer\n","import torchvision.models as models\n","from torchvision import transforms as T\n","from torch.nn.parameter import Parameter\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n","import torch.nn.init as init\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from albumentations import ImageOnlyTransform\n","\n","import timm\n","\n","from torch.cuda.amp import autocast, GradScaler\n","\n","import lightly\n","\n","import warnings \n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBJRDxNRmQcE"},"source":["NUM_EXP = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Wwpdedzlv1G"},"source":["INPUT_DIR = '/content/input/dataset_atmaCup11/'\n","OUTPUT_DIR =  f'/content/drive/MyDrive/atmacup/atmacup11/exp/exp{NUM_EXP:03}/'\n","\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yR4ArCvamws9"},"source":["train_df = pd.read_csv(INPUT_DIR + 'train.csv')\n","test_df = pd.read_csv(INPUT_DIR + 'test.csv')\n","\n","materials_df = pd.read_csv(INPUT_DIR + 'materials.csv')\n","techniques_df = pd.read_csv(INPUT_DIR + 'techniques.csv')\n","\n","train_df['image'] = train_df['object_id'].apply(lambda x: INPUT_DIR + 'photos/' + x + '.jpg')\n","test_df['image'] = test_df['object_id'].apply(lambda x: INPUT_DIR + 'photos/' + x + '.jpg')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WYiXP7mnFAi"},"source":["# CFG\n","## Configuration(設定) ,,. ここの設定を変えてモデルを変更する！\n","後方互換性があるらしい..."]},{"cell_type":"code","metadata":{"id":"TuQrnydAnBFO"},"source":["class CFG:\n","    apex=False  # always False\n","    debug=False\n","    print_freq=100\n","    num_workers=4\n","    num_gpu = torch.cuda.device_count()\n","    model_name = 'tf_efficientnet_b0_ns' # Pytorchのイメージモデル timm\n","    size=224   # 画像のサイズ\n","    scheduler='GradualWarmupSchedulerV2' # 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', GradualWarmupSchedulerV2\n","    epochs=25\n","    multi_gpu = False \n","    batch_size=32  # batch size per GPUs\n","\n","\n","    # GradualWarmupSchedulerV2\n","    scheduler_params = {'lr_start': 7.5e-6, 'min_lr': 1e-6, 'lr_max': 3.125e-6*batch_size*num_gpu if multi_gpu else 3.125e-6*batch_size }\n","    multiplier = scheduler_params['lr_max'] / scheduler_params['lr_start']\n","    warmup_epochs = 3\n","    cosine_epochs = epochs - warmup_epochs\n","\n","    min_lr=scheduler_params['min_lr']\n","\n","    #factor=0.2 # ReduceLROnPlateau\n","    #patience=4 # ReduceLROnPlateau\n","    #eps=1e-6 # ReduceLROnPlateau\n","    # T_max=6 # CosineAnnealingLR\n","    #T_0=6 # CosineAnnealingWarmRestarts\n","    optimizer = \"Ranger\"  # Adam, SGD, AdamW, Ranger,\n","    use_sam = False  # Optimizerのなんかすごいやつ\n","    optimizer_params = {'lr': scheduler_params['lr_max'],\n","                        'weight_decay':1e-6,\n","                        # 'momentum': 0.9,  # SGD\n","                        }\n","    # lr=1e-4\n","\n","    # augmentations\n","    augmentations = {\n","        'RandomResizedCrop': {'height': size, 'width': size},\n","        'Transpose': {'p': 0.5},\n","        'VerticalFlip': {'p': 0.5},\n","        'HorizontalFlip': {'p': 0.5},\n","\n","        'ShiftScaleRotate': {'p': 0.5},\n","        'HueSaturationValue': {'hue_shift_limit': 0.2, 'sat_shift_limit': 0.2, 'val_shift_limit': 0.2, 'p': 0.5},\n","        'RandomBrightnessContrast': {'brightness_limit': (-0.1, 0.1), 'contrast_limit': (-0.1, 0.1), 'p': 0.5}\n","    }\n","    use_course_dropout = True\n","    use_cutout = True\n","\n","    use_mixup = False  # 分類問題におけるデータAugmentations\n","    alpha = 1.0   # MixUpのハイパーパラメータ\n","\n","    # loss function\n","    # criterion = 'MSELoss'  # MSELoss, CrossEntroyLoss\n","\n","    # weight_decay=1e-6\n","    gradient_accumulation_steps=1  # バッチサイズがメモリー的に足りない場合に使う\n","    max_grad_norm=1000  # 勾配の最大値\n","    seed=42  #  乱数の初期値を固定\n","    # if criterion == 'CrossEntropyLoss':\n","    #     target_size = 4\n","    # else:\n","    #     target_size = 1\n","    \n","    # training target\n","    training_type = 'A'  # A, B, C\n","\n","    if training_type == 'A':  # target={0,1,2,3}を直接予測（回帰（SigmoidやSoftmaxなし））\n","        criterion = 'MSELoss'\n","        train_target = 'target'\n","        target_type = np.float32\n","    elif training_type == 'B':  # 年(スケーリングあり)の予測　（回帰)\n","        criterion = 'MSELoss'\n","        train_target = 'sorting_date'\n","        target_type = np.float32\n","    elif training_type == 'C':　#  分類問題\n","        criterion = 'CrossEntropyLoss'\n","        train_target = 'target'\n","        target_type = np.int32\n","\n","    # pretraining by materials.csv or techniques.csv\n","    pretraining = True\n","    pretrain_type = 'materials'  # materials or techniques\n","    if pretrain_type == 'materials':\n","        pretrain_target = ['cardboard', 'chalk', 'deck paint', 'gouache (paint)', 'graphite (mineral)', 'ink', 'oil paint (paint)', 'paint (coating)', 'paper', 'parchment (animal material)', 'pencil', 'prepared paper', 'tracing paper', 'watercolor (paint)']\n","        pretrain_target_type = np.float32\n","        pretrain_criterion = 'BCEWithLogitsLoss'\n","    elif pretrain_type == 'techniques':\n","        pretrain_target = ['brush', 'counterproof', 'pen']\n","        pretrain_target_type = np.float32\n","        pretrain_criterion = 'BCEWithLogitsLoss'\n","\n","    target_col = 'target'\n","\n","    # self supervised\n","    self_supervised = False\n","    self_supervised_method = 'SimSiam'\n","    pred_hidden_dim = 512\n","    out_dim = 512\n","    num_mlp_layers = 2\n","\n","    n_fold=4  # 交差検証法\n","    trn_fold=[0, 1, 2, 3]\n","    train=True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4beNNAY7nOlZ"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","if CFG.debug:\n","    CFG.epochs = 1\n","    train_df = train_df.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfjOVhwSnThO"},"source":["# Utils\n","$ get_score : RMSE $\n","$ init_logger : ログの出力 $"]},{"cell_type":"code","metadata":{"id":"jQud2YY3nSCA"},"source":["def get_score(y_true, y_pred):\n","    score = mean_squared_error(y_true, y_pred)\n","    return np.sqrt(score)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5EWkg9TsDG1"},"source":["def init_logger(log_file=OUTPUT_DIR+'train.log'):\n","    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOHjOrTIsGrz"},"source":["def seed_torch(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True  # False for Faster training\n","    torch.backends.cudnn.banchmark = False  # True for faster training\n","\n","seed_torch(seed=CFG.seed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"620tzUzty5z4"},"source":["# preprocessing\n","\n","$ データをfold単位でばらした交差検証法　$ \n"]},{"cell_type":"code","metadata":{"id":"FeUfy9lNpj7x"},"source":["# https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n","\n","def stratified_group_k_fold(X, y, groups, k, seed=None):\n","    labels_num = np.max(y) + 1\n","    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n","    y_distr = Counter()\n","    for label, g in zip(y, groups):\n","        y_counts_per_group[g][label] += 1\n","        y_distr[label] += 1\n","\n","    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n","    groups_per_fold = defaultdict(set)\n","\n","    def eval_y_counts_per_fold(y_counts, fold):\n","        y_counts_per_fold[fold] += y_counts\n","        std_per_label = []\n","        for label in range(labels_num):\n","            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n","            std_per_label.append(label_std)\n","        y_counts_per_fold[fold] -= y_counts\n","        return np.mean(std_per_label)\n","    \n","    groups_and_y_counts = list(y_counts_per_group.items())\n","    random.Random(seed).shuffle(groups_and_y_counts)\n","\n","    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n","        best_fold = None\n","        min_eval = None\n","        for i in range(k):\n","            fold_eval = eval_y_counts_per_fold(y_counts, i)\n","            if min_eval is None or fold_eval < min_eval:\n","                min_eval = fold_eval\n","                best_fold = i\n","        y_counts_per_fold[best_fold] += y_counts\n","        groups_per_fold[best_fold].add(g)\n","\n","    all_groups = set(groups)\n","    for i in range(k):\n","        train_groups = all_groups - groups_per_fold[i]\n","        test_groups = groups_per_fold[i]\n","\n","        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n","        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n","\n","        yield train_indices, test_indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzIjbNaxokmi"},"source":["$ materialとTechnicalのOneHot化をして、pretrainするため！$"]},{"cell_type":"code","metadata":{"id":"p1spC51ay8Nq"},"source":["def preprocess(train_df, materials_df, techniques_df):\n","    # 余分な列を切り落として、onehotにして、train_dfにマージする\n","    def merge(train_df, df):\n","        df_oh = pd.get_dummies(df, columns=['name'])\n","        df_oh = df_oh.set_index('object_id')\n","        df_oh = df_oh.sum(level=0).reset_index()\n","\n","        # 余分な列名削除\n","        new_name = {}\n","        for name in df_oh:\n","            new_name[name] = name.replace('name_', '')\n","\n","        df_oh = df_oh.rename(columns=new_name)\n","\n","        df_cols = [c for c in df_oh.loc[:, [True, *(df_oh[[col for col in df_oh.columns if col != 'object_id']].values.sum(axis=0) > 5)]].columns if c != 'object_id']\n","        # print(materials_cols)\n","        df_oh_ = df_oh.loc[:, ['object_id', *df_cols]]\n","\n","        train_df_merged = pd.merge(train_df, df_oh_, on='object_id', how='left').fillna(0.0)\n","        return train_df_merged\n","    df_merged = merge(train_df, materials_df)\n","    df_merged = merge(df_merged, techniques_df)\n","    return df_merged\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGJx4Mj3r_sa","executionInfo":{"status":"ok","timestamp":1626143963165,"user_tz":-540,"elapsed":2064,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"892a08a1-de7f-4926-f5b4-6f67bfe7c5be"},"source":["for fold, (train_idx, val_idx) in enumerate(stratified_group_k_fold(train_df['image'], train_df['target'], groups=train_df['art_series_id'], k=CFG.n_fold, seed=CFG.seed)):\n","    train_df.loc[val_idx, 'fold'] = int(fold)\n","\n","train_df['fold'] = train_df['fold'].astype(int)\n","train_df = preprocess(train_df, materials_df, techniques_df)\n","train_df.groupby(['fold', 'target']).size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fold  target\n","0     0         119\n","      1         224\n","      2         378\n","      3         264\n","1     0         119\n","      1         224\n","      2         378\n","      3         264\n","2     0         119\n","      1         224\n","      2         378\n","      3         264\n","3     0         118\n","      1         224\n","      2         377\n","      3         263\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"eSGu8Znktztl"},"source":["# dataset"]},{"cell_type":"code","metadata":{"id":"xF1INvbxtAEW"},"source":["def get_image(file_path):\n","    image = cv2.imread(file_path)[:, :, ::-1]\n","    # image = image.astype(np.float32)\n","    # image = np.vstack(image).transpose((1, 0))\n","    return image\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, file_names, labels=None, transform=None, return_dict=False, target_type=np.float32):\n","        \"\"\"\n","        Args:\n","            file_names\n","            labels\n","            transform\n","            return_dict  (bool): if True, __getitem__ returns dict type, else, returns tuple\n","        \"\"\"\n","        self.file_names = file_names\n","        self.labels = labels\n","        self.transform = transform\n","        self.return_dict = return_dict\n","        self.target_type = target_type\n","        \n","    def __len__(self):\n","        return len(self.file_names)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_names[idx]\n","        image = get_image(file_path)\n","        if self.transform:\n","            image = self.transform(image=image)['image']\n","        if self.labels is None:\n","            if self.return_dict:\n","                return {'image': image}\n","            else:\n","                return image\n","            \n","        else:\n","            label = np.array(self.labels[idx]).astype(self.target_type)\n","            if self.return_dict:\n","                return {'image': image, 'label': label}\n","            else:\n","                return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_bHeegvt16L"},"source":["# Transforms"]},{"cell_type":"code","metadata":{"id":"ab9ZN-jxtxKH"},"source":["def get_transforms(*, data):\n","    dropout = []\n","    if CFG.use_course_dropout:\n","        dropout.append(A.CoarseDropout(p=0.5))\n","    if CFG.use_cutout:\n","        dropout.append( A.Cutout(p=0.5))\n","    \n","    if data == 'train':\n","        return A.Compose([\n","            # A.Resize(CFG.size, CFG.size),\n","            *(getattr(A, aug)(**arg) for aug, arg in CFG.augmentations.items()),\n","            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","            *dropout,\n","            ToTensorV2(),\n","        ])\n","\n","    elif data == 'valid':\n","        return A.Compose([\n","            A.Resize(CFG.size, CFG.size),\n","            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","            ToTensorV2(),\n","        ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UXsdmaint9pG"},"source":["# Optimizer"]},{"cell_type":"code","metadata":{"id":"lKkSCLl9t-oz"},"source":["#credit : https://github.com/Yonghongwei/Gradient-Centralization\n","\n","def centralized_gradient(x, use_gc=True, gc_conv_only=False):\n","    if use_gc:\n","        if gc_conv_only:\n","            if len(list(x.size())) > 3:\n","                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n","        else:\n","            if len(list(x.size())) > 1:\n","                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n","    return x\n","\n","\n","class Ranger(Optimizer):\n","\n","    def __init__(self, params, lr=1e-3,                       # lr\n","                 alpha=0.5, k=5, N_sma_threshhold=5,           # Ranger options\n","                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n","                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n","                 use_gc=True, gc_conv_only=False, gc_loc=True\n","                 ):\n","\n","        # parameter checks\n","        if not 0.0 <= alpha <= 1.0:\n","            raise ValueError(f'Invalid slow update rate: {alpha}')\n","        if not 1 <= k:\n","            raise ValueError(f'Invalid lookahead steps: {k}')\n","        if not lr > 0:\n","            raise ValueError(f'Invalid Learning Rate: {lr}')\n","        if not eps > 0:\n","            raise ValueError(f'Invalid eps: {eps}')\n","\n","        # parameter comments:\n","        # beta1 (momentum) of .95 seems to work better than .90...\n","        # N_sma_threshold of 5 seems better in testing than 4.\n","        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n","\n","        # prep defaults and init torch.optim base\n","        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n","                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n","        super().__init__(params, defaults)\n","\n","        # adjustable threshold\n","        self.N_sma_threshhold = N_sma_threshhold\n","\n","        # look ahead params\n","\n","        self.alpha = alpha\n","        self.k = k\n","\n","        # radam buffer for state\n","        self.radam_buffer = [[None, None, None] for ind in range(10)]\n","\n","        # gc on or off\n","        self.gc_loc = gc_loc\n","        self.use_gc = use_gc\n","        self.gc_conv_only = gc_conv_only\n","        # level of gradient centralization\n","        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n","\n","        print(\n","            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n","        if (self.use_gc and self.gc_conv_only == False):\n","            print(f\"GC applied to both conv and fc layers\")\n","        elif (self.use_gc and self.gc_conv_only == True):\n","            print(f\"GC applied to conv layers only\")\n","\n","    def __setstate__(self, state):\n","        print(\"set state called\")\n","        super(Ranger, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        loss = None\n","        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n","        # Uncomment if you need to use the actual closure...\n","\n","        # if closure is not None:\n","        #loss = closure()\n","\n","        # Evaluate averages and grad, update param tensors\n","        for group in self.param_groups:\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data.float()\n","\n","                if grad.is_sparse:\n","                    raise RuntimeError(\n","                        'Ranger optimizer does not support sparse gradients')\n","\n","                p_data_fp32 = p.data.float()\n","\n","                state = self.state[p]  # get state dict for this param\n","\n","                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n","                    # if self.first_run_check==0:\n","                    # self.first_run_check=1\n","                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","\n","                    # look ahead weight storage now in state dict\n","                    state['slow_buffer'] = torch.empty_like(p.data)\n","                    state['slow_buffer'].copy_(p.data)\n","\n","                else:\n","                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n","                        p_data_fp32)\n","\n","                # begin computations\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                # GC operation for Conv layers and FC layers\n","                # if grad.dim() > self.gc_gradient_threshold:\n","                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n","                if self.gc_loc:\n","                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n","\n","                state['step'] += 1\n","\n","                # compute variance mov avg\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","\n","                # compute mean moving avg\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","\n","                buffered = self.radam_buffer[int(state['step'] % 10)]\n","\n","                if state['step'] == buffered[0]:\n","                    N_sma, step_size = buffered[1], buffered[2]\n","                else:\n","                    buffered[0] = state['step']\n","                    beta2_t = beta2 ** state['step']\n","                    N_sma_max = 2 / (1 - beta2) - 1\n","                    N_sma = N_sma_max - 2 * \\\n","                        state['step'] * beta2_t / (1 - beta2_t)\n","                    buffered[1] = N_sma\n","                    if N_sma > self.N_sma_threshhold:\n","                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n","                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n","                    else:\n","                        step_size = 1.0 / (1 - beta1 ** state['step'])\n","                    buffered[2] = step_size\n","\n","                # if group['weight_decay'] != 0:\n","                #    p_data_fp32.add_(-group['weight_decay']\n","                #                     * group['lr'], p_data_fp32)\n","\n","                # apply lr\n","                if N_sma > self.N_sma_threshhold:\n","                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n","                    G_grad = exp_avg / denom\n","                else:\n","                    G_grad = exp_avg\n","\n","                if group['weight_decay'] != 0:\n","                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n","                # GC operation\n","                if self.gc_loc == False:\n","                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n","\n","                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n","                p.data.copy_(p_data_fp32)\n","\n","                # integrated look ahead...\n","                # we do it at the param level instead of group level\n","                if state['step'] % group['k'] == 0:\n","                    # get access to slow param tensor\n","                    slow_p = state['slow_buffer']\n","                    # (fast weights - slow weights) * alpha\n","                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n","                    # copy interpolated weights to RAdam param tensor\n","                    p.data.copy_(slow_p)\n","\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dzdb9d8-uCKR"},"source":["class SAM(torch.optim.Optimizer):\n","    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n","        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n","\n","        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n","        super(SAM, self).__init__(params, defaults)\n","\n","        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n","        self.param_groups = self.base_optimizer.param_groups\n","\n","    @torch.no_grad()\n","    def first_step(self, zero_grad=False):\n","        grad_norm = self._grad_norm()\n","        for group in self.param_groups:\n","            scale = group[\"rho\"] / (grad_norm + 1e-12)\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n","                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n","                self.state[p][\"e_w\"] = e_w\n","\n","        if zero_grad: self.zero_grad()\n","\n","    @torch.no_grad()\n","    def second_step(self, zero_grad=False):\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n","\n","        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n","\n","        if zero_grad: self.zero_grad()\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n","        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n","\n","        self.first_step(zero_grad=True)\n","        closure()\n","        self.second_step()\n","\n","    def _grad_norm(self):\n","        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n","        norm = torch.norm(\n","                    torch.stack([\n","                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n","                        for group in self.param_groups for p in group[\"params\"]\n","                        if p.grad is not None\n","                    ]),\n","                    p=2\n","               )\n","        return norm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfGZK8R1uFJ0"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"2J8h1i34uEP5"},"source":["class CustomModel(nn.Module):\n","    def __init__(self, cfg, pretrained=False, backbone=None, target_size=1):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.model = timm.create_model(self.cfg.model_name, pretrained=False)\n","        if 'efficientnet' in self.cfg.model_name:\n","            self.n_features = self.model.classifier.in_features\n","            self.model.classifier = nn.Identity()\n","        elif 'resnet' in self.cfg.model_name:\n","            self.n_features = self.model.fc.in_features\n","            self.model.fc = nn.Identity()\n","        elif 'nfnet' in self.cfg.model_name:\n","            self.n_features = self.model.head.fc.in_features\n","            self.model.head.fc = nn.Identity()\n","        elif ('vit' in self.cfg.model_name) or ('swin' in self.cfg.model_name):\n","            self.n_features = self.model.head.in_features\n","            self.model.head = nn.Identity()\n","        \n","        if backbone is not None:\n","            self.model.load_state_dict(backbone.state_dict())\n","\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(self.n_features, target_size)\n","            )\n","        # self.n_features = self.model.head.fc.in_features\n","        # self.model.head.fc = nn.Linear(self.n_features, self.cfg.target_size)\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        output = self.fc(x)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvHgJXwpucYC"},"source":["# Scheduler\n","$ Gradual Wormup Schedulerです$"]},{"cell_type":"code","metadata":{"id":"CbGJxJIiuYlh"},"source":["from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","class GradualWarmupScheduler(_LRScheduler):\n","    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n","    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n","    Args:\n","        optimizer (Optimizer): Wrapped optimizer.\n","        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n","        total_epoch: target learning rate is reached at total_epoch, gradually\n","        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n","    \"\"\"\n","\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        self.multiplier = multiplier\n","        if self.multiplier < 1.:\n","            raise ValueError('multiplier should be greater thant or equal to 1.')\n","        self.total_epoch = total_epoch\n","        self.after_scheduler = after_scheduler\n","        self.finished = False\n","        super(GradualWarmupScheduler, self).__init__(optimizer)\n","\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_last_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n","        if self.last_epoch <= self.total_epoch:\n","            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n","                param_group['lr'] = lr\n","        else:\n","            if epoch is None:\n","                self.after_scheduler.step(metrics, None)\n","            else:\n","                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n","\n","    def step(self, epoch=None, metrics=None):\n","        if type(self.after_scheduler) != ReduceLROnPlateau:\n","            if self.finished and self.after_scheduler:\n","                if epoch is None:\n","                    self.after_scheduler.step(None)\n","                else:\n","                    self.after_scheduler.step(epoch - self.total_epoch)\n","                self._last_lr = self.after_scheduler.get_last_lr()\n","            else:\n","                return super(GradualWarmupScheduler, self).step(epoch)\n","        else:\n","            self.step_ReduceLROnPlateau(metrics, epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-O11nW4ugUR"},"source":["class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpiwsGEpuizG"},"source":["# Mixup\n","\n","$ （A + B） / 2 = の画像を取る$ "]},{"cell_type":"code","metadata":{"id":"iWeI0DXZuklS"},"source":["）def mixup_data(x, y, alpha=1.0,\n","    # use_cuda=True, device=\"cpu\"\n","    ):\n","\n","    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0.:\n","        lam = np.random.beta(alpha, alpha)\n","        lam = max(lam, 1-lam)\n","        # lam = min(lam, 1-lam)\n","    else:\n","        lam = 1.\n","    batch_size = x.size()[0]\n","\n","    # if use_cuda:\n","    #     index = torch.randperm(batch_size).to(device)\n","    # else:\n","    #     index = torch.randperm(batch_size)\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    ## SYM\n","    # mixed_x = lam * x + (1 - lam) * x[index,:]\n","    # mixed_y = (1 - lam) * x + lam * x[index,:]\n","    # mixed_image  = torch.cat([mixed_x,mixed_y], 0)\n","    # y_a, y_b = y, y[index]\n","    # mixed_label  = torch.cat([y_a,y_b], 0)\n","\n","\n","    ## Reduce batch size\n","    # new_batch_size = batch_size // 2\n","    # x_i = x[ : new_batch_size]\n","    # x_j = x[new_batch_size : ]\n","    # y_a = y[ : new_batch_size]\n","    # y_b = y[new_batch_size : ]\n","    # mixed_x = lam * x_i + (1 - lam) * x_j\n","\n","\n","    ## NO SYM\n","    mixed_x = lam * x + (1 - lam) * x[index,:]\n","    y_a, y_b = y, y[index]\n","\n","    ## Only Alpha\n","    # mixed_x = 0.5 * x + (1 - 0.5) * x[index,:]\n","    # mixed_image  = mixed_x\n","    # y_a, y_b = y, y[index]\n","    # ind_label = torch.randint_like(y, 0,2)\n","    # mixed_label  = ind_label * y_a + (1-ind_label) * y_b\n","\n","    ## Reduce batch size and SYM\n","    # new_batch_size = batch_size // 2\n","    # x_i = x[ : new_batch_size]\n","    # x_j = x[new_batch_size : ]\n","    # y_a = y[ : new_batch_size]\n","    # y_b = y[new_batch_size : ]\n","    # mixed_x = lam * x_i + (1 - lam) * x_j\n","    # mixed_y = (1 - lam) * x_i + lam * x_j\n","    # mixed_x  = torch.cat([mixed_x,mixed_y], 0)\n","    # y_b = torch.cat([y_b,y_a], 0)\n","    # y_a = y\n","\n","\n","    # return mixed_image, mixed_label, lam\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(y_a, y_b, lam):\n","    # sigmoid = 1.0/(1 + math.exp( 5 - 10*lam))\n","    # sigmoid = 4.67840515/(5.85074311 + math.exp(6.9-10.2120858*lam))\n","    # sigmoid = 1.531 /(1.71822 + math.exp(6.9-12.2836*lam))\n","    # return lambda criterion, pred: sigmoid * criterion(pred, y_a) + (1 - sigmoid) * criterion(pred, y_b)\n","\n","    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ib_542N6uoKu"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"oZyQoPj8unEd"},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdbO7u0dusmd"},"source":["def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, rank=None, world_size=None):\n","    if CFG.apex:\n","        scaler = GradScaler()\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    scores = AverageMeter()\n","    # switch to train mode\n","    model.train()\n","    start = end = time.time()\n","    global_step = 0\n","\n","    input_list, output_list, lams = [], [], []\n","\n","    for step, (images, labels) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        if CFG.multi_gpu:\n","            images = images.to(rank)\n","            labels = labels.to(rank)\n","        else:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","        batch_size = labels.size(0)\n","\n","        \n","\n","        if CFG.use_mixup:\n","            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=CFG.alpha)\n","\n","\n","        if CFG.apex:\n","            with autocast():\n","                y_preds = model(images)\n","                if CFG.use_mixup:\n","                    loss_fn = mixup_criterion(labels_a, labels_b, lam)\n","                    loss = loss_fn(criterion, y_preds)  # view\n","                else:\n","                    loss = criterion(y_preds, labels)  # view\n","        else:\n","            y_preds = model(images)\n","            if CFG.use_mixup:\n","                loss_fn = mixup_criterion(labels_a, labels_b, lam)\n","                loss = loss_fn(criterion, y_preds)  # view\n","            else:\n","                loss = criterion(y_preds, labels)  # view\n","        # record loss\n","        if (CFG.use_sam) and (CFG.gradient_accumulation_steps>1):\n","            input_list.append(images)\n","            \n","            if CFG.use_mixup:\n","                output_list.append([labels_a, labels_b])\n","                lams.append(lam)\n","            else:\n","                output_list.append(labels)\n","\n","        losses.update(loss.item(), batch_size)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        if CFG.apex and (not CFG.use_sam):\n","            scaler.scale(loss).backward()\n","        else:\n","            loss.backward()\n","\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            if CFG.apex:\n","                if CFG.use_sam:\n","                    optimizer.first_step(zero_grad=True)\n","                    # scaler.step(optimizer.first_step)\n","                    # scaler.update()\n","                    for i in range(len(input_list)):\n","                        with autocast():\n","                            pred =  model(input_list[i])\n","                        if CFG.use_mixup:\n","                            loss_fn = mixup_criterion(output_list[i][0], output_list[i][1], lams[i])\n","                            loss = loss_fn(criterion, y_preds)  # view\n","                        else:\n","                            loss = criterion(pred, output_list[i])\n","                        loss = loss / CFG.gradient_accumulation_steps\n","                        # scaler.scale(loss).backward()\n","                        loss.backward()\n","                    optimizer.second_step(zero_grad=True)\n","                    # scaler.step(optimizer.second_step)\n","                    # scaler.update()\n","                else:\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","            else:\n","                if CFG.use_sam:\n","                    optimizer.first_step(zero_grad=True)\n","                    y_preds = model(images)\n","                    if CFG.use_mixup:\n","                        loss_fn = mixup_criterion(labels_a, labels_b, lam)\n","                        loss = loss_fn(criterion, y_preds)  # view\n","                    else:\n","                        loss = criterion(y_preds, labels)  # view\n","                    loss.backward()\n","                    # loss.backward()\n","                    optimizer.second_step(zero_grad=True)  \n","                else:\n","                    optimizer.step()\n","            optimizer.zero_grad()\n","            global_step += 1\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  #'LR: {lr:.6f}  '\n","                  .format(\n","                   epoch+1, step, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses,\n","                   remain=timeSince(start, float(step+1)/len(train_loader)),\n","                   grad_norm=grad_norm,\n","                   #lr=scheduler.get_lr()[0],\n","                   ))\n","    return losses.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRykRlRnuwFP"},"source":["def valid_fn(valid_loader, model, criterion, device):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    scores = AverageMeter()\n","    # switch to evaluation mode\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (images, labels) in enumerate(valid_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        # compute loss\n","        with torch.no_grad():\n","            y_preds = model(images)\n","        loss = criterion(y_preds, labels)\n","        losses.update(loss.item(), batch_size)\n","        # record accuracy\n","        preds.append(y_preds.to('cpu').numpy())\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(\n","                   step, len(valid_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses,\n","                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n","                   ))\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyyRBskgqjpd"},"source":["# SSLのPretrainね！"]},{"cell_type":"code","metadata":{"id":"sx-NHHabHuky"},"source":["def pretrain_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, rank=None, world_size=None):\n","    if CFG.apex:\n","        scaler = GradScaler()\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    scores = AverageMeter()\n","    # switch to train mode\n","    model.train()\n","    start = end = time.time()\n","    global_step = 0\n","\n","    # input_list, output_list, lams = [], [], []\n","    avg_loss = 0.\n","    avg_output_std = 0.\n","    for step, ((x0, x1), _, _) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        x0 = x0.to(device)\n","        x1 = x1.to(device)\n","        batch_size = x0.size(0)\n","        \n","\n","\n","        with autocast(enabled=CFG.apex):\n","            y0, y1 = model(x0, x1)\n","        loss = criterion(y0, y1)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # calculate the per-dimension standard deviation of the outputs\n","        # we can use this later to check whether the embeddings are collapsing\n","        output, _ = y0\n","        output = output.detach()\n","        output = F.normalize(output, dim=1)\n","\n","        output_std = torch.std(output, 0)\n","        output_std = output_std.mean()\n","\n","        # use moving averages to track the loss and standard deviation\n","        w = 0.9\n","        avg_loss = w * avg_loss + (1 - w) * loss.item()\n","        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n","\n","        \n","\n","        losses.update(loss.item(), batch_size)\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                #   'Grad: {grad_norm:.4f}  '\n","                  #'LR: {lr:.6f}  '\n","                  .format(\n","                   epoch+1, step, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses,\n","                   remain=timeSince(start, float(step+1)/len(train_loader)),\n","                #    grad_norm=grad_norm,\n","                   #lr=scheduler.get_lr()[0],\n","                   ))\n","    # the level of collapse is large if the standard deviation of the l2\n","    # normalized output is much smaller than 1 / sqrt(dim)\n","    collapse_level = max(0., 1 - math.sqrt(CFG.out_dim) * avg_output_std)\n","    # print intermediate results\n","    LOGGER.info(f'[Epoch {epoch:3d}] '\n","        f'Loss = {avg_loss:.2f} | '\n","        f'Collapse Level: {collapse_level:.2f} / 1.00')\n","    return collapse_level"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nKd0tqxuzCB"},"source":["# Train loop"]},{"cell_type":"code","metadata":{"id":"wMKmzU-0uyTg"},"source":["def train_loop(folds, fold, backbone=None, phase='train'):\n","    \n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    trn_idx = folds[folds['fold'] != fold].index\n","    val_idx = folds[folds['fold'] == fold].index\n","\n","    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n","    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_col].values\n","\n","    if phase == 'train':\n","        target = CFG.train_target\n","        target_type = CFG.target_type\n","        if CFG.training_type == 'C':\n","            target_size = 4\n","        else:\n","            target_size = 1\n","        \n","    else:\n","        target = CFG.pretrain_target\n","        target_type = CFG.pretrain_target_type\n","        target_size = len(target)\n","\n","    train_target = train_folds[target].values\n","    valid_target = valid_folds[target].values\n","\n","\n","    if (CFG.training_type == 'B') and (phase == 'train'):\n","        train_target = (train_target - 1550) / 100\n","        valid_target = (valid_target - 1550) / 100\n","\n","\n","    # target_type = np.int32 if CFG.criterion == 'CrossEntropyLoss' else np.float32\n","\n","\n","\n","    train_dataset = ImageDataset(train_folds['image'].values, train_target, \n","                                 transform=get_transforms(data='train'), target_type=target_type)\n","    valid_dataset = ImageDataset(valid_folds['image'].values, valid_target, \n","                                 transform=get_transforms(data='valid'), target_type=target_type)\n","    if CFG.multi_gpu:\n","        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, rank=rank, shuffle=True)\n","\n","        train_loader = DataLoader(train_dataset, \n","                                  sampler=train_sampler,\n","                                  batch_size=CFG.batch_size, \n","                                  shuffle=False, \n","                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","        # valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, rank=rank)\n","\n","        valid_loader = DataLoader(valid_dataset, \n","                                #   sampler=valid_sampler,\n","                              batch_size=CFG.batch_size * 2, \n","                              shuffle=False, \n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    else:\n","        train_loader = DataLoader(train_dataset, \n","                                batch_size=CFG.batch_size, \n","                                shuffle=True, \n","                                num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","        valid_loader = DataLoader(valid_dataset, \n","                                batch_size=CFG.batch_size * 2, \n","                                shuffle=False, \n","                                num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    def get_optimizer(model):\n","        if CFG.optimizer == 'Adam':\n","            optimizer =  Adam\n","        elif CFG.optimizer == 'SGD':\n","            optimizer = SGD\n","        elif CFG.optimizer == 'AdamW':\n","            optimizer = AdamW\n","        elif CFG.optimizer == 'Ranger':\n","            optimizer = Ranger\n","        else:\n","            LOGGER.info(f'Optimizer {CFG.optimizer} is not implementated')\n","        \n","        if CFG.use_sam:\n","            return SAM(model.parameters(), optimizer, **CFG.optimizer_params)\n","        else:\n","            return optimizer(model.parameters(), **CFG.optimizer_params)\n","\n","\n","    # ====================================================\n","    # scheduler \n","    # ====================================================\n","    def get_scheduler(optimizer):\n","        if CFG.scheduler=='ReduceLROnPlateau':\n","            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n","        elif CFG.scheduler=='CosineAnnealingLR':\n","            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n","        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n","            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n","        elif CFG.scheduler == 'GradualWarmupSchedulerV2':\n","            scheduler_cosine = CosineAnnealingLR(optimizer, T_max=CFG.cosine_epochs - CFG.warmup_epochs, eta_min=CFG.min_lr, last_epoch=-1)\n","            scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=CFG.multiplier, total_epoch=CFG.warmup_epochs, after_scheduler=scheduler_cosine)\n","        else:\n","            LOGGER.info(f'Scheduler {CFG.scheduler} is not implementated')\n","        return scheduler\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, pretrained=False, backbone=backbone, target_size=target_size)\n","    if CFG.multi_gpu:\n","        model.to(rank)\n","        process_group = torch.distributed.new_group([i for i in range(world_size)])\n","        model = nn.SyncBatchNorm.convert_sync_batchnorm(model, process_group)\n","        model = DDP(model, device_ids=[rank])\n","    else:\n","        model.to(device)\n","\n","    optimizer = get_optimizer(model)\n","    scheduler = get_scheduler(optimizer)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    if phase == 'train':\n","        loss_fn = CFG.criterion\n","    else:\n","        loss_fn = CFG.pretrain_criterion\n","    criterion = getattr(nn, loss_fn)()\n","\n","    best_score = np.inf\n","    best_loss = np.inf\n","    \n","    for epoch in range(CFG.epochs):\n","        \n","        start_time = time.time()\n","        \n","        # train\n","        if CFG.multi_gpu:\n","            train_sampler.set_epoch(epoch)\n","        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","\n","        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n","        \n","        if isinstance(scheduler, ReduceLROnPlateau):\n","            scheduler.step(avg_val_loss)\n","        elif isinstance(scheduler, CosineAnnealingLR):\n","            scheduler.step()\n","        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","        elif isinstance(scheduler, GradualWarmupSchedulerV2):\n","            scheduler.step()\n","\n","        # scoring\n","        if phase == 'train':\n","            if CFG.training_type == 'C':\n","                score = get_score(valid_labels, np.argmax(preds, axis=1))\n","            else:\n","                score = get_score(valid_labels, preds)\n","        else:\n","            score = 0\n","\n","        elapsed = time.time() - start_time\n","        \n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        if phase == 'train':\n","            LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","\n","        if score < best_score:\n","            best_score = score\n","\n","            torch.save({'model': model.state_dict(), \n","                    'preds': preds},\n","                    OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{phase}_best_score.pth')\n","        \n","        if avg_val_loss < best_loss:\n","            best_loss = avg_val_loss\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n","                \n","            torch.save({'model': model.state_dict(), \n","                        'preds': preds},\n","                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{phase}_best_loss.pth')\n","\n","    if phase == 'train':\n","        if CFG.training_type == 'C':\n","            valid_folds['preds'] = np.argmax(torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{phase}_best_loss.pth', \n","                                            map_location=torch.device('cpu'))['preds'],\n","                                            axis=1)\n","        else:\n","            valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{phase}_best_loss.pth', \n","                                            map_location=torch.device('cpu'))['preds']\n","    else:\n","        saved = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{phase}_best_loss.pth', \n","                                            map_location=torch.device('cpu'))\n","        preds = saved['preds']\n","        for c, prd in zip(*(target, preds.T)):\n","            valid_folds[f'{c}_oof'] = prd\n","\n","        backbone = model.load_state_dict(saved['model'])\n","        backbone = model.model\n","        return valid_folds, backbone\n","\n","\n","    # \n","    # if CFG.multi_gpu and (rank == 0):\n","    #     with open(OUTPUT_DIR + f'valid_folds{fold}.pickle', 'wb') as f:\n","    #         pickle.dump(valid_folds, f)\n","    return valid_folds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCNmb28LEayC"},"source":["# Self Supervised Learning"]},{"cell_type":"code","metadata":{"id":"2qL-7XlXEeIa"},"source":["def pretrain_loop(train_df):\n","    # define the augmentations for self-supervised learning\n","    collate_fn = lightly.data.ImageCollateFunction(\n","        input_size=CFG.size,\n","        # require invariance to flips and rotations\n","        hf_prob=0.5,\n","        vf_prob=0.5,\n","        rr_prob=0.5,\n","        # satellite images are all taken from the same height\n","        # so we use only slight random cropping\n","        min_scale=0.5,\n","        # use a weak color jitter for invariance w.r.t small color changes\n","        cj_prob=0.2,\n","        cj_bright=0.1,\n","        cj_contrast=0.1,\n","        cj_hue=0.1,\n","        cj_sat=0.1,\n","    )\n","\n","    # create a lightly dataset for training, since the augmentations are handled\n","    # by the collate function, there is no need to apply additional ones here\n","    dataset_train_simsiam = lightly.data.LightlyDataset(\n","        input_dir=INPUT_DIR + 'photos/'\n","    )\n","\n","    # create a dataloader for training\n","    dataloader_train_simsiam = torch.utils.data.DataLoader(\n","        dataset_train_simsiam,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        collate_fn=collate_fn,\n","        drop_last=True,\n","        num_workers=CFG.num_workers\n","    )\n","\n","    # create a torchvision transformation for embedding the dataset after training\n","    # here, we resize the images to match the input size during training and apply\n","    # a normalization of the color channel based on statistics from imagenet\n","    test_transforms = T.Compose([\n","        T.Resize((CFG.size, CFG.size)),\n","        T.ToTensor(),\n","        T.Normalize(\n","            mean=lightly.data.collate.imagenet_normalize['mean'],\n","            std=lightly.data.collate.imagenet_normalize['std'],\n","        )\n","    ])\n","\n","    # create a lightly dataset for embedding\n","    dataset_test = lightly.data.LightlyDataset(\n","        input_dir=INPUT_DIR + 'photos/',\n","        transform=test_transforms\n","    )\n","    # create a dataloader for embedding\n","    dataloader_test = torch.utils.data.DataLoader(\n","        dataset_test,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        drop_last=False,\n","        num_workers=CFG.num_workers\n","    )\n","\n","    model = CustomModel(CFG, pretrained=False)\n","    num_ftrs = model.fc[1].in_features\n","    model.fc = nn.Identity()\n","    \n","    if CFG.self_supervised_method == 'SimSiam':\n","        model = lightly.models.SimSiam(\n","            model,\n","            num_ftrs=num_ftrs,\n","            proj_hidden_dim=CFG.pred_hidden_dim,\n","            pred_hidden_dim=CFG.pred_hidden_dim,\n","            out_dim=CFG.out_dim,\n","            num_mlp_layers=CFG.num_mlp_layers\n","        )\n","    else:\n","        model = getattr(lightly.models, CFG.self_supervised_method)(\n","            model,\n","            num_ftrs=num_ftrs,\n","            # proj_hidden_dim=CFG.pred_hidden_dim,\n","            # pred_hidden_dim=CFG.pred_hidden_dim,\n","            out_dim=CFG.out_dim,\n","            # num_mlp_layers=CFG.num_mlp_layers\n","        )\n","\n","    model.to(device)\n","    if CFG.self_supervised_method == 'SimSiam':\n","        # SimSiam uses a symmetric negative cosine similarity loss\n","        criterion = lightly.loss.SymNegCosineSimilarityLoss()\n","    else:\n","        criterion = lightly.loss.NTXentLoss()\n","\n","    optimizer = torch.optim.SGD(\n","        model.parameters(),\n","        lr=0.05 * CFG.batch_size / 256,\n","        momentum=0.9,\n","        weight_decay=5e-4\n","    )\n","    # ====================================================\n","    # scheduler \n","    # ====================================================\n","    def get_scheduler(optimizer):\n","        if CFG.scheduler=='ReduceLROnPlateau':\n","            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n","        elif CFG.scheduler=='CosineAnnealingLR':\n","            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n","        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n","            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n","        elif CFG.scheduler == 'GradualWarmupSchedulerV2':\n","            scheduler_cosine = CosineAnnealingLR(optimizer, T_max=CFG.cosine_epochs - CFG.warmup_epochs, eta_min=CFG.min_lr, last_epoch=-1)\n","            scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=CFG.multiplier, total_epoch=CFG.warmup_epochs, after_scheduler=scheduler_cosine)\n","        else:\n","            LOGGER.info(f'Scheduler {CFG.scheduler} is not implementated')\n","        return scheduler\n","    scheduler = get_scheduler(optimizer)\n","\n","    best_level = 1.0\n","    for epoch in range(CFG.epochs):\n","        \n","        start_time = time.time()\n","        \n","        avg_collapse_level = pretrain_fn(dataloader_train_simsiam, model, criterion, optimizer, epoch, scheduler, device)\n","\n","\n","        if isinstance(scheduler, ReduceLROnPlateau):\n","            scheduler.step(avg_val_loss)\n","        elif isinstance(scheduler, CosineAnnealingLR):\n","            scheduler.step()\n","        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n","            scheduler.step()\n","        elif isinstance(scheduler, GradualWarmupSchedulerV2):\n","            scheduler.step()\n","\n","        if avg_collapse_level < best_level:\n","            best_level = avg_collapse_level\n","\n","            torch.save(model.backbone.model.state_dict(), \n","                        OUTPUT_DIR+f'{CFG.model_name}_{CFG.self_supervised_method}_best_collapse_level.pth')\n","    \n","        \n","    return model.backbone.model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6K2tB7JvWjG"},"source":["# Inference"]},{"cell_type":"code","metadata":{"id":"75hNX-fQvTnH"},"source":["def inference(model, states, test_loader, device):\n","    model.to(device)\n","    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n","    probs = []\n","    for i, (images) in tk0:\n","        images = images.to(device)\n","        avg_preds = []\n","        for state in states:\n","            model.load_state_dict(state['model'])\n","            model.eval()\n","            with torch.no_grad():\n","                y_preds = model(images)\n","            avg_preds.append(y_preds.to('cpu').numpy())\n","        avg_preds = np.mean(avg_preds, axis=0)\n","        probs.append(avg_preds)\n","    probs = np.concatenate(probs)\n","    return probs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_J8p93z6va2K"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"ymjhHRE7vZ-I"},"source":["def main():\n","\n","    \"\"\"\n","    Prepare: 1.train \n","    \"\"\"\n","\n","    LOGGER.info(\"======= start =========\")\n","\n","    def get_result(result_df):\n","        preds = result_df['preds'].values\n","        labels = result_df[CFG.target_col].values\n","        score = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:<.4f}')\n","    \n","    if CFG.train:\n","        # train \n","        if CFG.self_supervised:\n","            backbone = pretrain_loop(train_df)\n","        else:\n","            backbone = None\n","\n","        \n","\n","        oof_df = pd.DataFrame()\n","        if CFG.pretraining:\n","            df = pd.DataFrame()\n","\n","\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                \n","                if CFG.pretraining:\n","                    LOGGER.info(f'========== fold {fold} {CFG.pretrain_type} ================')\n","                    _df, backbone_ = train_loop(train_df, fold, backbone=backbone, phase='pretrain')\n","                    df = pd.concat([df, _df])\n","                _oof_df = train_loop(train_df, fold, backbone=backbone_, phase='train')\n","                oof_df = pd.concat([oof_df, _oof_df])\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        # CV result\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        # save result\n","        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n","        if CFG.pretraining:\n","            df.to_csv(OUTPUT_DIR + f'{CFG.pretrain_type}_oof.csv', index=False)\n","\n","        LOGGER.info('========== Inference =========')\n","        # ====================================================\n","        # inference\n","        # ====================================================\n","        model = CustomModel(CFG, pretrained=False)\n","        MODEL_DIR = OUTPUT_DIR\n","        states = [torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_train_best_loss.pth') for fold in CFG.trn_fold]\n","        test_dataset = ImageDataset(test_df['image'].values, transform=get_transforms(data='valid'))\n","        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n","                                num_workers=CFG.num_workers, pin_memory=True)\n","        predictions = inference(model, states, test_loader, device)\n","\n","        # submission\n","        if CFG.training_type == 'C':\n","            for i in range(4):\n","                test_df[f'{i}'] = predictions[:, i]\n","            predictions = np.argmax(predictions, axis=1)\n","        test_df['target'] = predictions\n","        test_df[['target']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1608208d63bc412681ba66f593440c57","4cd7dcc1a0d14dac9e80b280929b7103","67aabadae20b45b3bdbfb270fe9c77cf","a21ac66472e14d83832a86736e5b6c1b","cff12f5892f2454d85a91c44e6ec7858","b55fc452e85346519ed3d456766e479e","5af03550542d47609394a81549809e68","2b44ef4ec6e14ae484960bffddfbe080","1f414cf6a39d49f08872d4db4de03352","f0c02e4ab5db417a96b378de86743005","4f4f8bccae2343ba928685ec72339f01"]},"id":"fcHbRg0gvlXH","executionInfo":{"status":"ok","timestamp":1626148579242,"user_tz":-540,"elapsed":4614977,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"c8c4448b-7f36-4dc4-88df-ac95a0e2a03f"},"source":["if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======= start =========\n","========== fold 0 materials ================\n","========== fold: 0 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.441 (0.441) Elapsed 0m 1s (remain 1m 42s) Loss: 0.6994(0.6994) Grad: 0.9225  \n","Epoch: [1][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6933(0.6943) Grad: 0.9749  \n","EVAL: [0/16] Data 0.696 (0.696) Elapsed 0m 0s (remain 0m 11s) Loss: 0.7242(0.7242) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.6943  avg_val_loss: 0.7594  time: 22s\n","Epoch 1 - Save Best Loss: 0.7594 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 0.7469(0.7594) \n","Epoch: [2][0/92] Data 0.356 (0.356) Elapsed 0m 0s (remain 1m 1s) Loss: 0.6854(0.6854) Grad: 0.8758  \n","Epoch: [2][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6505(0.6787) Grad: 1.0401  \n","EVAL: [0/16] Data 0.631 (0.631) Elapsed 0m 0s (remain 0m 10s) Loss: 0.6996(0.6996) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.6787  avg_val_loss: 0.7176  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.7077(0.7176) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - Save Best Loss: 0.7176 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [3][0/92] Data 0.709 (0.709) Elapsed 0m 1s (remain 1m 34s) Loss: 0.6604(0.6604) Grad: 1.0957  \n","Epoch: [3][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.4427(0.5692) Grad: 0.7816  \n","EVAL: [0/16] Data 0.874 (0.874) Elapsed 0m 0s (remain 0m 14s) Loss: 0.4648(0.4648) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5692  avg_val_loss: 0.4980  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 0.5024(0.4980) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - Save Best Loss: 0.4980 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [4][0/92] Data 0.476 (0.476) Elapsed 0m 0s (remain 1m 12s) Loss: 0.4550(0.4550) Grad: 23.4764  \n","Epoch: [4][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2163(0.2950) Grad: 0.2809  \n","EVAL: [0/16] Data 0.628 (0.628) Elapsed 0m 0s (remain 0m 10s) Loss: 0.3366(0.3366) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.2950  avg_val_loss: 0.5110  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 0.6510(0.5110) \n","Epoch: [5][0/92] Data 0.558 (0.558) Elapsed 0m 0s (remain 1m 18s) Loss: 0.2321(0.2321) Grad: 0.4441  \n","Epoch: [5][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2189(0.2279) Grad: 0.2820  \n","EVAL: [0/16] Data 0.778 (0.778) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2626(0.2626) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.2279  avg_val_loss: 0.2630  time: 22s\n","Epoch 5 - Save Best Loss: 0.2630 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.087) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3272(0.2630) \n","Epoch: [6][0/92] Data 0.514 (0.514) Elapsed 0m 0s (remain 1m 10s) Loss: 0.2308(0.2308) Grad: 0.7587  \n","Epoch: [6][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1954(0.2226) Grad: 1.1227  \n","EVAL: [0/16] Data 0.669 (0.669) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2998(0.2998) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.2226  avg_val_loss: 0.3540  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3890(0.3540) \n","Epoch: [7][0/92] Data 0.380 (0.380) Elapsed 0m 0s (remain 1m 5s) Loss: 0.2059(0.2059) Grad: 0.3719  \n","Epoch: [7][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2185(0.2207) Grad: 0.3440  \n","EVAL: [0/16] Data 0.529 (0.529) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2626(0.2626) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.2207  avg_val_loss: 0.2915  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3191(0.2915) \n","Epoch: [8][0/92] Data 0.588 (0.588) Elapsed 0m 0s (remain 1m 22s) Loss: 0.1718(0.1718) Grad: 0.3432  \n","Epoch: [8][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2479(0.2162) Grad: 0.7363  \n","EVAL: [0/16] Data 0.756 (0.756) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2633(0.2633) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.2162  avg_val_loss: 0.3011  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.079) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3088(0.3011) \n","Epoch: [9][0/92] Data 0.588 (0.588) Elapsed 0m 0s (remain 1m 22s) Loss: 0.2532(0.2532) Grad: 0.3807  \n","Epoch: [9][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1986(0.2124) Grad: 0.4198  \n","EVAL: [0/16] Data 0.680 (0.680) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2885(0.2885) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.2124  avg_val_loss: 0.3325  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3227(0.3325) \n","Epoch: [10][0/92] Data 0.555 (0.555) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2613(0.2613) Grad: 1.3496  \n","Epoch: [10][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2200(0.2113) Grad: 0.8074  \n","EVAL: [0/16] Data 0.647 (0.647) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2424(0.2424) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.2113  avg_val_loss: 0.2731  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2863(0.2731) \n","Epoch: [11][0/92] Data 0.608 (0.608) Elapsed 0m 0s (remain 1m 23s) Loss: 0.2561(0.2561) Grad: 0.3712  \n","Epoch: [11][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1717(0.2088) Grad: 0.3733  \n","EVAL: [0/16] Data 0.640 (0.640) Elapsed 0m 0s (remain 0m 10s) Loss: 0.3440(0.3440) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.2088  avg_val_loss: 0.3601  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3460(0.3601) \n","Epoch: [12][0/92] Data 0.515 (0.515) Elapsed 0m 0s (remain 1m 15s) Loss: 0.1769(0.1769) Grad: 0.3427  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1796(0.2054) Grad: 0.3655  \n","EVAL: [0/16] Data 0.673 (0.673) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2494(0.2494) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.2054  avg_val_loss: 0.2692  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3236(0.2692) \n","Epoch: [13][0/92] Data 0.570 (0.570) Elapsed 0m 0s (remain 1m 19s) Loss: 0.1897(0.1897) Grad: 0.5859  \n","Epoch: [13][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2189(0.2046) Grad: 0.5401  \n","EVAL: [0/16] Data 0.666 (0.666) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2222(0.2222) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.2046  avg_val_loss: 0.2681  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.065) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2648(0.2681) \n","Epoch: [14][0/92] Data 0.478 (0.478) Elapsed 0m 0s (remain 1m 11s) Loss: 0.2000(0.2000) Grad: 0.3079  \n","Epoch: [14][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1966(0.2019) Grad: 0.4299  \n","EVAL: [0/16] Data 0.834 (0.834) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2326(0.2326) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.2019  avg_val_loss: 0.2685  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.079) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2621(0.2685) \n","Epoch: [15][0/92] Data 0.514 (0.514) Elapsed 0m 0s (remain 1m 14s) Loss: 0.1969(0.1969) Grad: 0.4694  \n","Epoch: [15][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2478(0.2000) Grad: 0.5503  \n","EVAL: [0/16] Data 0.650 (0.650) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2214(0.2214) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.2000  avg_val_loss: 0.2525  time: 22s\n","Epoch 15 - Save Best Loss: 0.2525 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2661(0.2525) \n","Epoch: [16][0/92] Data 0.589 (0.589) Elapsed 0m 0s (remain 1m 20s) Loss: 0.2381(0.2381) Grad: 0.7535  \n","Epoch: [16][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1679(0.1993) Grad: 0.2905  \n","EVAL: [0/16] Data 0.733 (0.733) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2143(0.2143) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.1993  avg_val_loss: 0.2541  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2538(0.2541) \n","Epoch: [17][0/92] Data 0.443 (0.443) Elapsed 0m 0s (remain 1m 8s) Loss: 0.1752(0.1752) Grad: 0.6362  \n","Epoch: [17][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1789(0.1985) Grad: 0.5448  \n","EVAL: [0/16] Data 0.701 (0.701) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2379(0.2379) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.1985  avg_val_loss: 0.2531  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2528(0.2531) \n","Epoch: [18][0/92] Data 0.409 (0.409) Elapsed 0m 0s (remain 1m 12s) Loss: 0.2284(0.2284) Grad: 0.4406  \n","Epoch: [18][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1999(0.1966) Grad: 0.3409  \n","EVAL: [0/16] Data 0.587 (0.587) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2317(0.2317) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.1966  avg_val_loss: 0.2559  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2544(0.2559) \n","Epoch: [19][0/92] Data 0.587 (0.587) Elapsed 0m 0s (remain 1m 24s) Loss: 0.1556(0.1556) Grad: 0.3360  \n","Epoch: [19][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2476(0.1947) Grad: 0.5198  \n","EVAL: [0/16] Data 0.615 (0.615) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2119(0.2119) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.1947  avg_val_loss: 0.2266  time: 22s\n","Epoch 19 - Save Best Loss: 0.2266 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2510(0.2266) \n","Epoch: [20][0/92] Data 0.586 (0.586) Elapsed 0m 0s (remain 1m 20s) Loss: 0.2274(0.2274) Grad: 0.3834  \n","Epoch: [20][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1714(0.1940) Grad: 0.4848  \n","EVAL: [0/16] Data 0.771 (0.771) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2114(0.2114) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.1940  avg_val_loss: 0.2251  time: 22s\n","Epoch 20 - Save Best Loss: 0.2251 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2520(0.2251) \n","Epoch: [21][0/92] Data 0.638 (0.638) Elapsed 0m 0s (remain 1m 28s) Loss: 0.1893(0.1893) Grad: 0.5085  \n","Epoch: [21][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1821(0.1934) Grad: 0.4492  \n","EVAL: [0/16] Data 0.637 (0.637) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2182(0.2182) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.1934  avg_val_loss: 0.2361  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2503(0.2361) \n","Epoch: [22][0/92] Data 0.389 (0.389) Elapsed 0m 0s (remain 1m 4s) Loss: 0.2204(0.2204) Grad: 0.4638  \n","Epoch: [22][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2079(0.1931) Grad: 0.4771  \n","EVAL: [0/16] Data 0.617 (0.617) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2095(0.2095) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.1931  avg_val_loss: 0.2247  time: 22s\n","Epoch 22 - Save Best Loss: 0.2247 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2539(0.2247) \n","Epoch: [23][0/92] Data 0.591 (0.591) Elapsed 0m 0s (remain 1m 22s) Loss: 0.2427(0.2427) Grad: 0.5590  \n","Epoch: [23][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1848(0.1943) Grad: 0.7405  \n","EVAL: [0/16] Data 0.572 (0.572) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2096(0.2096) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.1943  avg_val_loss: 0.2309  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2528(0.2309) \n","Epoch: [24][0/92] Data 0.434 (0.434) Elapsed 0m 0s (remain 1m 6s) Loss: 0.1897(0.1897) Grad: 0.3980  \n","Epoch: [24][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1843(0.1930) Grad: 0.3943  \n","EVAL: [0/16] Data 0.764 (0.764) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2137(0.2137) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.1930  avg_val_loss: 0.2325  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.083) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2532(0.2325) \n","Epoch: [25][0/92] Data 0.533 (0.533) Elapsed 0m 0s (remain 1m 16s) Loss: 0.1888(0.1888) Grad: 0.4267  \n","Epoch: [25][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1695(0.1913) Grad: 0.4131  \n","EVAL: [0/16] Data 0.670 (0.670) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2149(0.2149) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.1913  avg_val_loss: 0.2443  time: 22s\n","========== fold: 0 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2525(0.2443) \n","Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.615 (0.615) Elapsed 0m 0s (remain 1m 22s) Loss: 4.8798(4.8798) Grad: 28.3007  \n","Epoch: [1][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 3.6937(4.1138) Grad: 23.9139  \n","EVAL: [0/16] Data 0.678 (0.678) Elapsed 0m 0s (remain 0m 11s) Loss: 6.6606(6.6606) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 4.1138  avg_val_loss: 7.6856  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 3.5451(7.6856) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - Score: 2.7574\n","Epoch 1 - Save Best Loss: 7.6856 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [2][0/92] Data 0.418 (0.418) Elapsed 0m 0s (remain 1m 11s) Loss: 3.5052(3.5052) Grad: 23.1573  \n","Epoch: [2][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2665(1.9231) Grad: 4.7631  \n","EVAL: [0/16] Data 0.639 (0.639) Elapsed 0m 0s (remain 0m 11s) Loss: 3.6772(3.6772) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 1.9231  avg_val_loss: 3.9119  time: 22s\n","Epoch 2 - Score: 1.9759\n","Epoch 2 - Save Best Loss: 3.9119 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1589(3.9119) \n","Epoch: [3][0/92] Data 0.647 (0.647) Elapsed 0m 0s (remain 1m 23s) Loss: 1.0832(1.0832) Grad: 2.5687  \n","Epoch: [3][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0906(0.9674) Grad: 1.4892  \n","EVAL: [0/16] Data 0.682 (0.682) Elapsed 0m 0s (remain 0m 11s) Loss: 1.8879(1.8879) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.9674  avg_val_loss: 1.6224  time: 22s\n","Epoch 3 - Score: 1.2683\n","Epoch 3 - Save Best Loss: 1.6224 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0104(1.6224) \n","Epoch: [4][0/92] Data 0.606 (0.606) Elapsed 0m 0s (remain 1m 26s) Loss: 0.7349(0.7349) Grad: 9.8028  \n","Epoch: [4][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9182(0.9534) Grad: 2.8760  \n","EVAL: [0/16] Data 0.782 (0.782) Elapsed 0m 0s (remain 0m 13s) Loss: 1.7291(1.7291) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.9534  avg_val_loss: 2.1859  time: 22s\n","Epoch 4 - Score: 1.4813\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0078(2.1859) \n","Epoch: [5][0/92] Data 0.601 (0.601) Elapsed 0m 0s (remain 1m 23s) Loss: 1.0362(1.0362) Grad: 0.4323  \n","Epoch: [5][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0067(0.9500) Grad: 1.3848  \n","EVAL: [0/16] Data 0.776 (0.776) Elapsed 0m 0s (remain 0m 12s) Loss: 2.7726(2.7726) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.9500  avg_val_loss: 4.2078  time: 22s\n","Epoch 5 - Score: 2.0492\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 1.6384(4.2078) \n","Epoch: [6][0/92] Data 0.511 (0.511) Elapsed 0m 0s (remain 1m 13s) Loss: 1.0231(1.0231) Grad: 1.8751  \n","Epoch: [6][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 1.0693(0.9514) Grad: 2.1216  \n","EVAL: [0/16] Data 0.559 (0.559) Elapsed 0m 0s (remain 0m 9s) Loss: 0.8513(0.8513) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.9514  avg_val_loss: 1.2450  time: 22s\n","Epoch 6 - Score: 1.1107\n","Epoch 6 - Save Best Loss: 1.2450 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0104(1.2450) \n","Epoch: [7][0/92] Data 0.493 (0.493) Elapsed 0m 0s (remain 1m 14s) Loss: 1.0650(1.0650) Grad: 2.9052  \n","Epoch: [7][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0209(0.9496) Grad: 4.5391  \n","EVAL: [0/16] Data 0.753 (0.753) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9926(0.9926) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.9496  avg_val_loss: 1.5245  time: 22s\n","Epoch 7 - Score: 1.2397\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0231(1.5245) \n","Epoch: [8][0/92] Data 0.557 (0.557) Elapsed 0m 0s (remain 1m 27s) Loss: 0.9546(0.9546) Grad: 0.5003  \n","Epoch: [8][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7523(0.9494) Grad: 1.6202  \n","EVAL: [0/16] Data 0.767 (0.767) Elapsed 0m 0s (remain 0m 13s) Loss: 0.6945(0.6945) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.9494  avg_val_loss: 0.9857  time: 22s\n","Epoch 8 - Score: 0.9963\n","Epoch 8 - Save Best Loss: 0.9857 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0122(0.9857) \n","Epoch: [9][0/92] Data 0.576 (0.576) Elapsed 0m 0s (remain 1m 21s) Loss: 0.9652(0.9652) Grad: 1.6280  \n","Epoch: [9][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9351(0.9557) Grad: 0.9757  \n","EVAL: [0/16] Data 0.678 (0.678) Elapsed 0m 0s (remain 0m 11s) Loss: 0.7191(0.7191) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.9557  avg_val_loss: 1.0479  time: 22s\n","Epoch 9 - Score: 1.0172\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0872(1.0479) \n","Epoch: [10][0/92] Data 0.536 (0.536) Elapsed 0m 0s (remain 1m 21s) Loss: 1.1400(1.1400) Grad: 0.6395  \n","Epoch: [10][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2745(0.9517) Grad: 1.8408  \n","EVAL: [0/16] Data 0.680 (0.680) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8822(0.8822) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.9517  avg_val_loss: 1.0755  time: 22s\n","Epoch 10 - Score: 1.0066\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1916(1.0755) \n","Epoch: [11][0/92] Data 0.579 (0.579) Elapsed 0m 0s (remain 1m 16s) Loss: 0.9678(0.9678) Grad: 1.8938  \n","Epoch: [11][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.7299(0.9500) Grad: 1.1362  \n","EVAL: [0/16] Data 0.607 (0.607) Elapsed 0m 0s (remain 0m 10s) Loss: 0.7013(0.7013) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.9500  avg_val_loss: 0.9423  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0035(0.9423) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - Score: 0.9713\n","Epoch 11 - Save Best Loss: 0.9423 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [12][0/92] Data 0.615 (0.615) Elapsed 0m 0s (remain 1m 23s) Loss: 0.8488(0.8488) Grad: 3.7413  \n","Epoch: [12][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1025(0.9482) Grad: 3.7173  \n","EVAL: [0/16] Data 0.566 (0.566) Elapsed 0m 0s (remain 0m 9s) Loss: 0.6840(0.6840) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.9482  avg_val_loss: 0.9529  time: 22s\n","Epoch 12 - Score: 0.9672\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.083) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0361(0.9529) \n","Epoch: [13][0/92] Data 0.604 (0.604) Elapsed 0m 0s (remain 1m 24s) Loss: 1.2516(1.2516) Grad: 2.5972  \n","Epoch: [13][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8112(0.9494) Grad: 0.9664  \n","EVAL: [0/16] Data 0.850 (0.850) Elapsed 0m 0s (remain 0m 14s) Loss: 0.7055(0.7055) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.9494  avg_val_loss: 0.9475  time: 22s\n","Epoch 13 - Score: 0.9689\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.086) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0499(0.9475) \n","Epoch: [14][0/92] Data 0.582 (0.582) Elapsed 0m 0s (remain 1m 21s) Loss: 0.7260(0.7260) Grad: 0.7698  \n","Epoch: [14][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9599(0.9462) Grad: 1.6179  \n","EVAL: [0/16] Data 0.721 (0.721) Elapsed 0m 0s (remain 0m 12s) Loss: 0.6929(0.6929) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.9462  avg_val_loss: 0.9410  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0309(0.9410) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - Score: 0.9727\n","Epoch 14 - Save Best Loss: 0.9410 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [15][0/92] Data 0.596 (0.596) Elapsed 0m 0s (remain 1m 25s) Loss: 0.8106(0.8106) Grad: 0.7827  \n","Epoch: [15][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6812(0.9482) Grad: 0.8237  \n","EVAL: [0/16] Data 0.639 (0.639) Elapsed 0m 0s (remain 0m 10s) Loss: 0.6903(0.6903) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.9482  avg_val_loss: 0.9395  time: 22s\n","Epoch 15 - Score: 0.9665\n","Epoch 15 - Save Best Loss: 0.9395 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0091(0.9395) \n","Epoch: [16][0/92] Data 0.599 (0.599) Elapsed 0m 0s (remain 1m 23s) Loss: 0.8002(0.8002) Grad: 0.2379  \n","Epoch: [16][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7900(0.9455) Grad: 1.8755  \n","EVAL: [0/16] Data 0.658 (0.658) Elapsed 0m 0s (remain 0m 11s) Loss: 3.1032(3.1032) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.9455  avg_val_loss: 5.2943  time: 22s\n","Epoch 16 - Score: 2.3033\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 3.4899(5.2943) \n","Epoch: [17][0/92] Data 0.637 (0.637) Elapsed 0m 0s (remain 1m 24s) Loss: 1.0673(1.0673) Grad: 1.5819  \n","Epoch: [17][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0369(0.9460) Grad: 2.9139  \n","EVAL: [0/16] Data 0.792 (0.792) Elapsed 0m 0s (remain 0m 13s) Loss: 0.8666(0.8666) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.9460  avg_val_loss: 1.2650  time: 22s\n","Epoch 17 - Score: 1.1255\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0318(1.2650) \n","Epoch: [18][0/92] Data 0.461 (0.461) Elapsed 0m 0s (remain 1m 10s) Loss: 0.9628(0.9628) Grad: 1.5699  \n","Epoch: [18][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8464(0.9434) Grad: 1.3383  \n","EVAL: [0/16] Data 0.735 (0.735) Elapsed 0m 0s (remain 0m 12s) Loss: 1.1735(1.1735) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.9434  avg_val_loss: 1.9703  time: 22s\n","Epoch 18 - Score: 1.4074\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2846(1.9703) \n","Epoch: [19][0/92] Data 0.667 (0.667) Elapsed 0m 0s (remain 1m 20s) Loss: 1.1581(1.1581) Grad: 0.2819  \n","Epoch: [19][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.4235(0.9461) Grad: 4.7755  \n","EVAL: [0/16] Data 0.628 (0.628) Elapsed 0m 0s (remain 0m 10s) Loss: 0.7594(0.7594) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.9461  avg_val_loss: 1.1832  time: 22s\n","Epoch 19 - Score: 1.0867\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0470(1.1832) \n","Epoch: [20][0/92] Data 0.477 (0.477) Elapsed 0m 0s (remain 1m 9s) Loss: 0.8476(0.8476) Grad: 0.5917  \n","Epoch: [20][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0370(0.9443) Grad: 1.5374  \n","EVAL: [0/16] Data 0.840 (0.840) Elapsed 0m 0s (remain 0m 13s) Loss: 1.0910(1.0910) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.9443  avg_val_loss: 1.8477  time: 22s\n","Epoch 20 - Score: 1.3628\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2673(1.8477) \n","Epoch: [21][0/92] Data 0.480 (0.480) Elapsed 0m 0s (remain 1m 16s) Loss: 0.8406(0.8406) Grad: 2.8703  \n","Epoch: [21][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9325(0.9455) Grad: 1.4326  \n","EVAL: [0/16] Data 0.632 (0.632) Elapsed 0m 0s (remain 0m 10s) Loss: 0.7909(0.7909) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.9455  avg_val_loss: 1.1986  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0395(1.1986) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - Score: 1.0969\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [22][0/92] Data 0.603 (0.603) Elapsed 0m 0s (remain 1m 26s) Loss: 0.9828(0.9828) Grad: 2.1870  \n","Epoch: [22][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9116(0.9434) Grad: 3.5634  \n","EVAL: [0/16] Data 0.824 (0.824) Elapsed 0m 0s (remain 0m 13s) Loss: 0.8632(0.8632) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.9434  avg_val_loss: 1.3749  time: 22s\n","Epoch 22 - Score: 1.1751\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0836(1.3749) \n","Epoch: [23][0/92] Data 0.495 (0.495) Elapsed 0m 0s (remain 1m 14s) Loss: 1.0078(1.0078) Grad: 4.0054  \n","Epoch: [23][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2389(0.9425) Grad: 0.4729  \n","EVAL: [0/16] Data 0.739 (0.739) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8398(0.8398) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.9425  avg_val_loss: 1.3128  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0679(1.3128) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - Score: 1.1497\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [24][0/92] Data 0.546 (0.546) Elapsed 0m 0s (remain 1m 16s) Loss: 0.9178(0.9178) Grad: 3.7161  \n","Epoch: [24][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.7783(0.9444) Grad: 3.6314  \n","EVAL: [0/16] Data 0.634 (0.634) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8047(0.8047) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.9444  avg_val_loss: 1.2528  time: 22s\n","Epoch 24 - Score: 1.1231\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.066) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0530(1.2528) \n","Epoch: [25][0/92] Data 0.412 (0.412) Elapsed 0m 0s (remain 1m 4s) Loss: 1.0665(1.0665) Grad: 1.6088  \n","Epoch: [25][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8903(0.9428) Grad: 0.2783  \n","EVAL: [0/16] Data 0.636 (0.636) Elapsed 0m 0s (remain 0m 10s) Loss: 0.7783(0.7783) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.9428  avg_val_loss: 1.1850  time: 22s\n","Epoch 25 - Score: 1.0909\n","========== fold: 0 result ==========\n","Score: 0.9665\n","========== fold 1 materials ================\n","========== fold: 1 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.066) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0445(1.1850) \n","Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.539 (0.539) Elapsed 0m 0s (remain 1m 14s) Loss: 0.6823(0.6823) Grad: 0.8995  \n","Epoch: [1][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6833(0.6881) Grad: 0.8333  \n","EVAL: [0/16] Data 0.657 (0.657) Elapsed 0m 0s (remain 0m 11s) Loss: 0.6929(0.6929) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.6881  avg_val_loss: 0.7196  time: 22s\n","Epoch 1 - Save Best Loss: 0.7196 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 0.7026(0.7196) \n","Epoch: [2][0/92] Data 0.658 (0.658) Elapsed 0m 1s (remain 1m 31s) Loss: 0.6895(0.6895) Grad: 0.8507  \n","Epoch: [2][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6663(0.6729) Grad: 0.9532  \n","EVAL: [0/16] Data 0.543 (0.543) Elapsed 0m 0s (remain 0m 9s) Loss: 0.7090(0.7090) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.6729  avg_val_loss: 0.7728  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 0.7478(0.7728) \n","Epoch: [3][0/92] Data 0.675 (0.675) Elapsed 0m 0s (remain 1m 22s) Loss: 0.6563(0.6563) Grad: 0.8752  \n","Epoch: [3][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.4498(0.5669) Grad: 0.6326  \n","EVAL: [0/16] Data 0.659 (0.659) Elapsed 0m 0s (remain 0m 11s) Loss: 0.4508(0.4508) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5669  avg_val_loss: 0.4709  time: 22s\n","Epoch 3 - Save Best Loss: 0.4709 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.4860(0.4709) \n","Epoch: [4][0/92] Data 0.608 (0.608) Elapsed 0m 0s (remain 1m 18s) Loss: 0.4389(0.4389) Grad: 3.2063  \n","Epoch: [4][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2288(0.2954) Grad: 0.2541  \n","EVAL: [0/16] Data 0.631 (0.631) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2298(0.2298) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.2954  avg_val_loss: 0.2780  time: 22s\n","Epoch 4 - Save Best Loss: 0.2780 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2692(0.2780) \n","Epoch: [5][0/92] Data 0.662 (0.662) Elapsed 0m 1s (remain 1m 33s) Loss: 0.2471(0.2471) Grad: 0.3423  \n","Epoch: [5][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1806(0.2344) Grad: 0.2340  \n","EVAL: [0/16] Data 0.892 (0.892) Elapsed 0m 0s (remain 0m 14s) Loss: 0.2128(0.2128) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.2344  avg_val_loss: 0.2594  time: 22s\n","Epoch 5 - Save Best Loss: 0.2594 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.083) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2945(0.2594) \n","Epoch: [6][0/92] Data 0.503 (0.503) Elapsed 0m 0s (remain 1m 16s) Loss: 0.2124(0.2124) Grad: 0.2264  \n","Epoch: [6][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2283(0.2298) Grad: 0.2672  \n","EVAL: [0/16] Data 0.680 (0.680) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2098(0.2098) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.2298  avg_val_loss: 0.2379  time: 22s\n","Epoch 6 - Save Best Loss: 0.2379 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2561(0.2379) \n","Epoch: [7][0/92] Data 0.490 (0.490) Elapsed 0m 0s (remain 1m 12s) Loss: 0.2773(0.2773) Grad: 0.3229  \n","Epoch: [7][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1799(0.2247) Grad: 0.3664  \n","EVAL: [0/16] Data 0.527 (0.527) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2077(0.2077) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.2247  avg_val_loss: 0.2204  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2574(0.2204) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - Save Best Loss: 0.2204 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [8][0/92] Data 0.541 (0.541) Elapsed 0m 0s (remain 1m 17s) Loss: 0.2232(0.2232) Grad: 0.3613  \n","Epoch: [8][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2300(0.2237) Grad: 0.3882  \n","EVAL: [0/16] Data 0.675 (0.675) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2043(0.2043) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.2237  avg_val_loss: 0.2213  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3078(0.2213) \n","Epoch: [9][0/92] Data 0.620 (0.620) Elapsed 0m 0s (remain 1m 20s) Loss: 0.2166(0.2166) Grad: 0.2756  \n","Epoch: [9][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2155(0.2206) Grad: 0.2697  \n","EVAL: [0/16] Data 0.548 (0.548) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2104(0.2104) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.2206  avg_val_loss: 0.2179  time: 22s\n","Epoch 9 - Save Best Loss: 0.2179 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2577(0.2179) \n","Epoch: [10][0/92] Data 0.607 (0.607) Elapsed 0m 0s (remain 1m 23s) Loss: 0.1954(0.1954) Grad: 0.2544  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2634(0.2172) Grad: 0.4026  \n","EVAL: [0/16] Data 0.673 (0.673) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2016(0.2016) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.2172  avg_val_loss: 0.2111  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2471(0.2111) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - Save Best Loss: 0.2111 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [11][0/92] Data 0.613 (0.613) Elapsed 0m 0s (remain 1m 25s) Loss: 0.2131(0.2131) Grad: 0.2782  \n","Epoch: [11][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2301(0.2168) Grad: 0.2952  \n","EVAL: [0/16] Data 0.582 (0.582) Elapsed 0m 0s (remain 0m 9s) Loss: 0.1987(0.1987) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.2168  avg_val_loss: 0.2105  time: 22s\n","Epoch 11 - Save Best Loss: 0.2105 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2411(0.2105) \n","Epoch: [12][0/92] Data 0.501 (0.501) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2029(0.2029) Grad: 0.2454  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2159(0.2141) Grad: 0.3637  \n","EVAL: [0/16] Data 0.713 (0.713) Elapsed 0m 0s (remain 0m 11s) Loss: 0.1953(0.1953) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.2141  avg_val_loss: 0.2129  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2844(0.2129) \n","Epoch: [13][0/92] Data 0.521 (0.521) Elapsed 0m 0s (remain 1m 25s) Loss: 0.2116(0.2116) Grad: 0.4079  \n","Epoch: [13][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1974(0.2124) Grad: 0.3660  \n","EVAL: [0/16] Data 0.619 (0.619) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1965(0.1965) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.2124  avg_val_loss: 0.2104  time: 22s\n","Epoch 13 - Save Best Loss: 0.2104 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.3165(0.2104) \n","Epoch: [14][0/92] Data 0.471 (0.471) Elapsed 0m 0s (remain 1m 8s) Loss: 0.1938(0.1938) Grad: 0.4825  \n","Epoch: [14][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2012(0.2095) Grad: 0.4773  \n","EVAL: [0/16] Data 0.485 (0.485) Elapsed 0m 0s (remain 0m 8s) Loss: 0.1883(0.1883) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.2095  avg_val_loss: 0.2021  time: 22s\n","Epoch 14 - Save Best Loss: 0.2021 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2469(0.2021) \n","Epoch: [15][0/92] Data 0.530 (0.530) Elapsed 0m 0s (remain 1m 17s) Loss: 0.2149(0.2149) Grad: 0.5071  \n","Epoch: [15][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1911(0.2059) Grad: 0.3341  \n","EVAL: [0/16] Data 0.621 (0.621) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1906(0.1906) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.2059  avg_val_loss: 0.2046  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2321(0.2046) \n","Epoch: [16][0/92] Data 0.534 (0.534) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2177(0.2177) Grad: 0.2782  \n","Epoch: [16][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.1806(0.2052) Grad: 0.4176  \n","EVAL: [0/16] Data 0.558 (0.558) Elapsed 0m 0s (remain 0m 9s) Loss: 0.1857(0.1857) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.2052  avg_val_loss: 0.1988  time: 22s\n","Epoch 16 - Save Best Loss: 0.1988 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.065) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2192(0.1988) \n","Epoch: [17][0/92] Data 0.529 (0.529) Elapsed 0m 0s (remain 1m 17s) Loss: 0.2016(0.2016) Grad: 0.3349  \n","Epoch: [17][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1819(0.2035) Grad: 0.4730  \n","EVAL: [0/16] Data 0.794 (0.794) Elapsed 0m 0s (remain 0m 13s) Loss: 0.1818(0.1818) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.2035  avg_val_loss: 0.1983  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2293(0.1983) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - Save Best Loss: 0.1983 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [18][0/92] Data 0.457 (0.457) Elapsed 0m 0s (remain 1m 14s) Loss: 0.2398(0.2398) Grad: 0.3217  \n","Epoch: [18][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2353(0.2016) Grad: 0.5921  \n","EVAL: [0/16] Data 0.738 (0.738) Elapsed 0m 0s (remain 0m 12s) Loss: 0.1840(0.1840) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.2016  avg_val_loss: 0.1977  time: 22s\n","Epoch 18 - Save Best Loss: 0.1977 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2181(0.1977) \n","Epoch: [19][0/92] Data 0.613 (0.613) Elapsed 0m 0s (remain 1m 21s) Loss: 0.1997(0.1997) Grad: 0.2711  \n","Epoch: [19][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2426(0.2008) Grad: 0.4517  \n","EVAL: [0/16] Data 0.807 (0.807) Elapsed 0m 0s (remain 0m 13s) Loss: 0.1812(0.1812) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.2008  avg_val_loss: 0.1955  time: 22s\n","Epoch 19 - Save Best Loss: 0.1955 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2259(0.1955) \n","Epoch: [20][0/92] Data 0.510 (0.510) Elapsed 0m 0s (remain 1m 14s) Loss: 0.2349(0.2349) Grad: 0.6178  \n","Epoch: [20][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2163(0.2011) Grad: 0.4174  \n","EVAL: [0/16] Data 0.747 (0.747) Elapsed 0m 0s (remain 0m 12s) Loss: 0.1822(0.1822) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.2011  avg_val_loss: 0.1956  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2184(0.1956) \n","Epoch: [21][0/92] Data 0.405 (0.405) Elapsed 0m 0s (remain 1m 6s) Loss: 0.1671(0.1671) Grad: 0.3111  \n","Epoch: [21][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2182(0.1978) Grad: 0.3647  \n","EVAL: [0/16] Data 0.674 (0.674) Elapsed 0m 0s (remain 0m 11s) Loss: 0.1826(0.1826) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.1978  avg_val_loss: 0.1948  time: 22s\n","Epoch 21 - Save Best Loss: 0.1948 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2217(0.1948) \n","Epoch: [22][0/92] Data 0.660 (0.660) Elapsed 0m 0s (remain 1m 26s) Loss: 0.1867(0.1867) Grad: 0.4540  \n","Epoch: [22][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1933(0.1969) Grad: 0.3643  \n","EVAL: [0/16] Data 0.595 (0.595) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1812(0.1812) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.1969  avg_val_loss: 0.1949  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2233(0.1949) \n","Epoch: [23][0/92] Data 0.610 (0.610) Elapsed 0m 0s (remain 1m 23s) Loss: 0.1928(0.1928) Grad: 0.4387  \n","Epoch: [23][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2098(0.1991) Grad: 0.4631  \n","EVAL: [0/16] Data 0.595 (0.595) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1808(0.1808) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.1991  avg_val_loss: 0.1940  time: 22s\n","Epoch 23 - Save Best Loss: 0.1940 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2250(0.1940) \n","Epoch: [24][0/92] Data 0.466 (0.466) Elapsed 0m 0s (remain 1m 11s) Loss: 0.2145(0.2145) Grad: 0.3207  \n","Epoch: [24][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2102(0.1977) Grad: 0.4589  \n","EVAL: [0/16] Data 0.576 (0.576) Elapsed 0m 0s (remain 0m 9s) Loss: 0.1807(0.1807) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.1977  avg_val_loss: 0.1941  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2235(0.1941) \n","Epoch: [25][0/92] Data 0.609 (0.609) Elapsed 0m 0s (remain 1m 25s) Loss: 0.1947(0.1947) Grad: 0.3427  \n","Epoch: [25][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2227(0.1986) Grad: 0.4002  \n","EVAL: [0/16] Data 0.739 (0.739) Elapsed 0m 0s (remain 0m 12s) Loss: 0.1821(0.1821) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.1986  avg_val_loss: 0.1949  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2236(0.1949) \n"],"name":"stdout"},{"output_type":"stream","text":["========== fold: 1 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.480 (0.480) Elapsed 0m 0s (remain 1m 12s) Loss: 4.5610(4.5610) Grad: 27.0299  \n","Epoch: [1][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 3.6561(4.1221) Grad: 22.0907  \n","EVAL: [0/16] Data 0.643 (0.643) Elapsed 0m 0s (remain 0m 10s) Loss: 3.8107(3.8107) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 4.1221  avg_val_loss: 3.8801  time: 22s\n","Epoch 1 - Score: 1.9588\n","Epoch 1 - Save Best Loss: 3.8801 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 3.9838(3.8801) \n","Epoch: [2][0/92] Data 0.505 (0.505) Elapsed 0m 0s (remain 1m 11s) Loss: 4.4193(4.4193) Grad: 26.5876  \n","Epoch: [2][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.4066(1.8705) Grad: 4.1301  \n","EVAL: [0/16] Data 0.754 (0.754) Elapsed 0m 0s (remain 0m 12s) Loss: 1.0773(1.0773) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 1.8705  avg_val_loss: 1.1560  time: 22s\n","Epoch 2 - Score: 1.0762\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.4823(1.1560) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - Save Best Loss: 1.1560 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [3][0/92] Data 0.418 (0.418) Elapsed 0m 0s (remain 1m 10s) Loss: 1.1149(1.1149) Grad: 7.1006  \n","Epoch: [3][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1888(0.9703) Grad: 2.0387  \n","EVAL: [0/16] Data 0.748 (0.748) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9303(0.9303) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.9703  avg_val_loss: 1.0035  time: 22s\n","Epoch 3 - Score: 1.0051\n","Epoch 3 - Save Best Loss: 1.0035 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3358(1.0035) \n","Epoch: [4][0/92] Data 0.412 (0.412) Elapsed 0m 0s (remain 1m 6s) Loss: 1.0985(1.0985) Grad: 1.7727  \n","Epoch: [4][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8447(0.9527) Grad: 2.8834  \n","EVAL: [0/16] Data 0.758 (0.758) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9257(0.9257) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.9527  avg_val_loss: 1.0306  time: 22s\n","Epoch 4 - Score: 1.0149\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2920(1.0306) \n","Epoch: [5][0/92] Data 0.454 (0.454) Elapsed 0m 0s (remain 1m 13s) Loss: 1.0140(1.0140) Grad: 0.9100  \n","Epoch: [5][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2411(0.9517) Grad: 1.7204  \n","EVAL: [0/16] Data 0.698 (0.698) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9260(0.9260) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.9517  avg_val_loss: 0.9712  time: 22s\n","Epoch 5 - Score: 0.9781\n","Epoch 5 - Save Best Loss: 0.9712 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2170(0.9712) \n","Epoch: [6][0/92] Data 0.568 (0.568) Elapsed 0m 0s (remain 1m 23s) Loss: 0.9985(0.9985) Grad: 0.9194  \n","Epoch: [6][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9492(0.9522) Grad: 3.8484  \n","EVAL: [0/16] Data 0.678 (0.678) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9241(0.9241) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.9522  avg_val_loss: 0.9483  time: 22s\n","Epoch 6 - Score: 0.9727\n","Epoch 6 - Save Best Loss: 0.9483 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1984(0.9483) \n","Epoch: [7][0/92] Data 0.391 (0.391) Elapsed 0m 0s (remain 1m 4s) Loss: 0.8466(0.8466) Grad: 0.8129  \n","Epoch: [7][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8826(0.9518) Grad: 0.4859  \n","EVAL: [0/16] Data 0.851 (0.851) Elapsed 0m 0s (remain 0m 14s) Loss: 0.9300(0.9300) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.9518  avg_val_loss: 0.9849  time: 22s\n","Epoch 7 - Score: 0.9837\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2911(0.9849) \n","Epoch: [8][0/92] Data 0.476 (0.476) Elapsed 0m 0s (remain 1m 13s) Loss: 0.8667(0.8667) Grad: 2.3764  \n","Epoch: [8][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 1.2598(0.9513) Grad: 0.6061  \n","EVAL: [0/16] Data 0.670 (0.670) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9265(0.9265) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.9513  avg_val_loss: 0.9586  time: 22s\n","Epoch 8 - Score: 0.9742\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1864(0.9586) \n","Epoch: [9][0/92] Data 0.641 (0.641) Elapsed 0m 0s (remain 1m 27s) Loss: 0.8850(0.8850) Grad: 0.6752  \n","Epoch: [9][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2925(0.9509) Grad: 4.3288  \n","EVAL: [0/16] Data 0.838 (0.838) Elapsed 0m 0s (remain 0m 13s) Loss: 0.9239(0.9239) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.9509  avg_val_loss: 1.0505  time: 22s\n","Epoch 9 - Score: 1.0302\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2751(1.0505) \n","Epoch: [10][0/92] Data 0.594 (0.594) Elapsed 0m 0s (remain 1m 20s) Loss: 1.0816(1.0816) Grad: 2.0311  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9106(0.9474) Grad: 0.6370  \n","EVAL: [0/16] Data 0.529 (0.529) Elapsed 0m 0s (remain 0m 9s) Loss: 0.9241(0.9241) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.9474  avg_val_loss: 1.3718  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3460(1.3718) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - Score: 1.1673\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [11][0/92] Data 0.489 (0.489) Elapsed 0m 0s (remain 1m 11s) Loss: 0.8418(0.8418) Grad: 2.5621  \n","Epoch: [11][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8216(0.9479) Grad: 1.4303  \n","EVAL: [0/16] Data 0.647 (0.647) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9334(0.9334) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.9479  avg_val_loss: 1.3054  time: 22s\n","Epoch 11 - Score: 1.1392\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.4188(1.3054) \n","Epoch: [12][0/92] Data 0.383 (0.383) Elapsed 0m 0s (remain 1m 3s) Loss: 1.0233(1.0233) Grad: 0.9460  \n","Epoch: [12][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 1.2876(0.9511) Grad: 0.7706  \n","EVAL: [0/16] Data 0.713 (0.713) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9438(0.9438) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.9511  avg_val_loss: 0.9749  time: 22s\n","Epoch 12 - Score: 0.9995\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3406(0.9749) \n","Epoch: [13][0/92] Data 0.517 (0.517) Elapsed 0m 0s (remain 1m 19s) Loss: 0.4495(0.4495) Grad: 0.2124  \n","Epoch: [13][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8907(0.9466) Grad: 0.6364  \n","EVAL: [0/16] Data 0.804 (0.804) Elapsed 0m 0s (remain 0m 13s) Loss: 0.9542(0.9542) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.9466  avg_val_loss: 0.9561  time: 22s\n","Epoch 13 - Score: 0.9795\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2969(0.9561) \n","Epoch: [14][0/92] Data 0.564 (0.564) Elapsed 0m 0s (remain 1m 18s) Loss: 0.8218(0.8218) Grad: 0.5508  \n","Epoch: [14][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8183(0.9479) Grad: 0.3257  \n","EVAL: [0/16] Data 0.624 (0.624) Elapsed 0m 0s (remain 0m 10s) Loss: 1.1946(1.1946) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.9479  avg_val_loss: 1.3659  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 2.1413(1.3659) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - Score: 1.2100\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [15][0/92] Data 0.470 (0.470) Elapsed 0m 0s (remain 1m 12s) Loss: 0.6879(0.6879) Grad: 3.4335  \n","Epoch: [15][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.9588(0.9483) Grad: 2.1675  \n","EVAL: [0/16] Data 0.812 (0.812) Elapsed 0m 0s (remain 0m 13s) Loss: 1.1737(1.1737) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.9483  avg_val_loss: 1.3259  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.081) Elapsed 0m 2s (remain 0m 0s) Loss: 2.0636(1.3259) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - Score: 1.1839\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [16][0/92] Data 0.491 (0.491) Elapsed 0m 0s (remain 1m 12s) Loss: 0.8874(0.8874) Grad: 0.8472  \n","Epoch: [16][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8654(0.9450) Grad: 0.5382  \n","EVAL: [0/16] Data 0.645 (0.645) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9620(0.9620) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.9450  avg_val_loss: 1.2170  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2362(1.2170) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - Score: 1.1309\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [17][0/92] Data 0.589 (0.589) Elapsed 0m 0s (remain 1m 18s) Loss: 0.8465(0.8465) Grad: 0.5698  \n","Epoch: [17][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 1.0543(0.9487) Grad: 0.8756  \n","EVAL: [0/16] Data 0.827 (0.827) Elapsed 0m 0s (remain 0m 13s) Loss: 1.4454(1.4454) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.9487  avg_val_loss: 1.3240  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.081) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2024(1.3240) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - Score: 1.1797\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [18][0/92] Data 0.491 (0.491) Elapsed 0m 0s (remain 1m 13s) Loss: 0.8874(0.8874) Grad: 0.8451  \n","Epoch: [18][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8883(0.9436) Grad: 1.0988  \n","EVAL: [0/16] Data 0.569 (0.569) Elapsed 0m 0s (remain 0m 9s) Loss: 1.4001(1.4001) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.9436  avg_val_loss: 1.0762  time: 22s\n","Epoch 18 - Score: 1.0424\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2079(1.0762) \n","Epoch: [19][0/92] Data 0.601 (0.601) Elapsed 0m 0s (remain 1m 17s) Loss: 0.8475(0.8475) Grad: 2.3189  \n","Epoch: [19][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.9716(0.9450) Grad: 0.2758  \n","EVAL: [0/16] Data 0.503 (0.503) Elapsed 0m 0s (remain 0m 8s) Loss: 1.0395(1.0395) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.9450  avg_val_loss: 0.9649  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.081) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1936(0.9649) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - Score: 0.9821\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [20][0/92] Data 0.588 (0.588) Elapsed 0m 0s (remain 1m 20s) Loss: 1.3477(1.3477) Grad: 2.2598  \n","Epoch: [20][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2764(0.9442) Grad: 0.8695  \n","EVAL: [0/16] Data 0.737 (0.737) Elapsed 0m 0s (remain 0m 12s) Loss: 1.5302(1.5302) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.9442  avg_val_loss: 1.1425  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2008(1.1425) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - Score: 1.0777\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [21][0/92] Data 0.614 (0.614) Elapsed 0m 0s (remain 1m 27s) Loss: 1.0909(1.0909) Grad: 0.9964  \n","Epoch: [21][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7415(0.9451) Grad: 3.8906  \n","EVAL: [0/16] Data 0.635 (0.635) Elapsed 0m 0s (remain 0m 10s) Loss: 1.0860(1.0860) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.9451  avg_val_loss: 0.9810  time: 22s\n","Epoch 21 - Score: 0.9924\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1917(0.9810) \n","Epoch: [22][0/92] Data 0.585 (0.585) Elapsed 0m 0s (remain 1m 19s) Loss: 0.8211(0.8211) Grad: 0.4555  \n","Epoch: [22][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9709(0.9448) Grad: 0.9678  \n","EVAL: [0/16] Data 0.644 (0.644) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9847(0.9847) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.9448  avg_val_loss: 0.9532  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2187(0.9532) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - Score: 0.9721\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [23][0/92] Data 0.483 (0.483) Elapsed 0m 0s (remain 1m 12s) Loss: 0.8826(0.8826) Grad: 0.5605  \n","Epoch: [23][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7553(0.9437) Grad: 0.4969  \n","EVAL: [0/16] Data 0.783 (0.783) Elapsed 0m 0s (remain 0m 13s) Loss: 1.0971(1.0971) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.9437  avg_val_loss: 0.9919  time: 22s\n","Epoch 23 - Score: 0.9961\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2001(0.9919) \n","Epoch: [24][0/92] Data 0.570 (0.570) Elapsed 0m 0s (remain 1m 19s) Loss: 1.2952(1.2952) Grad: 2.5012  \n","Epoch: [24][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9268(0.9428) Grad: 0.2999  \n","EVAL: [0/16] Data 0.681 (0.681) Elapsed 0m 0s (remain 0m 11s) Loss: 1.2260(1.2260) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.9428  avg_val_loss: 1.0539  time: 22s\n","Epoch 24 - Score: 1.0296\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2260(1.0539) \n","Epoch: [25][0/92] Data 0.414 (0.414) Elapsed 0m 0s (remain 1m 4s) Loss: 0.7127(0.7127) Grad: 0.9107  \n","Epoch: [25][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2785(0.9439) Grad: 3.2347  \n","EVAL: [0/16] Data 0.776 (0.776) Elapsed 0m 0s (remain 0m 12s) Loss: 1.0777(1.0777) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.9439  avg_val_loss: 0.9730  time: 22s\n","Epoch 25 - Score: 0.9868\n","========== fold: 1 result ==========\n","Score: 0.9727\n","========== fold 2 materials ================\n","========== fold: 2 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1969(0.9730) \n","Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.531 (0.531) Elapsed 0m 0s (remain 1m 20s) Loss: 0.7002(0.7002) Grad: 0.8747  \n","Epoch: [1][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.7005(0.7023) Grad: 0.8939  \n","EVAL: [0/16] Data 0.812 (0.812) Elapsed 0m 0s (remain 0m 13s) Loss: 0.7190(0.7190) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.7023  avg_val_loss: 0.7646  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 0.6966(0.7646) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - Save Best Loss: 0.7646 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [2][0/92] Data 0.444 (0.444) Elapsed 0m 0s (remain 1m 10s) Loss: 0.7007(0.7007) Grad: 1.2337  \n","Epoch: [2][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6802(0.6911) Grad: 0.8355  \n","EVAL: [0/16] Data 0.619 (0.619) Elapsed 0m 0s (remain 0m 11s) Loss: 0.7075(0.7075) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.6911  avg_val_loss: 0.7388  time: 23s\n","Epoch 2 - Save Best Loss: 0.7388 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.6765(0.7388) \n","Epoch: [3][0/92] Data 0.503 (0.503) Elapsed 0m 0s (remain 1m 15s) Loss: 0.6801(0.6801) Grad: 1.0298  \n","Epoch: [3][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.4514(0.5840) Grad: 0.6114  \n","EVAL: [0/16] Data 0.598 (0.598) Elapsed 0m 0s (remain 0m 10s) Loss: 0.4309(0.4309) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5840  avg_val_loss: 0.4270  time: 22s\n","Epoch 3 - Save Best Loss: 0.4270 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.4213(0.4270) \n","Epoch: [4][0/92] Data 0.508 (0.508) Elapsed 0m 0s (remain 1m 11s) Loss: 0.4379(0.4379) Grad: 1.5026  \n","Epoch: [4][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2275(0.2915) Grad: 0.2572  \n","EVAL: [0/16] Data 0.581 (0.581) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2421(0.2421) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.2915  avg_val_loss: 0.2336  time: 22s\n","Epoch 4 - Save Best Loss: 0.2336 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2232(0.2336) \n","Epoch: [5][0/92] Data 0.616 (0.616) Elapsed 0m 0s (remain 1m 21s) Loss: 0.2511(0.2511) Grad: 0.2382  \n","Epoch: [5][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.2344(0.2339) Grad: 0.3655  \n","EVAL: [0/16] Data 0.644 (0.644) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2350(0.2350) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.2339  avg_val_loss: 0.2221  time: 22s\n","Epoch 5 - Save Best Loss: 0.2221 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2041(0.2221) \n","Epoch: [6][0/92] Data 0.584 (0.584) Elapsed 0m 0s (remain 1m 20s) Loss: 0.2420(0.2420) Grad: 0.2950  \n","Epoch: [6][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2098(0.2283) Grad: 0.1914  \n","EVAL: [0/16] Data 0.563 (0.563) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2339(0.2339) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.2283  avg_val_loss: 0.2211  time: 22s\n","Epoch 6 - Save Best Loss: 0.2211 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.065) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2039(0.2211) \n","Epoch: [7][0/92] Data 0.480 (0.480) Elapsed 0m 0s (remain 1m 10s) Loss: 0.2304(0.2304) Grad: 0.2321  \n","Epoch: [7][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2062(0.2262) Grad: 0.2451  \n","EVAL: [0/16] Data 0.865 (0.865) Elapsed 0m 0s (remain 0m 14s) Loss: 0.2279(0.2279) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.2262  avg_val_loss: 0.2148  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1996(0.2148) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - Save Best Loss: 0.2148 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [8][0/92] Data 0.477 (0.477) Elapsed 0m 0s (remain 1m 14s) Loss: 0.2082(0.2082) Grad: 0.2310  \n","Epoch: [8][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2102(0.2231) Grad: 0.2476  \n","EVAL: [0/16] Data 0.814 (0.814) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2241(0.2241) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.2231  avg_val_loss: 0.2117  time: 22s\n","Epoch 8 - Save Best Loss: 0.2117 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2024(0.2117) \n","Epoch: [9][0/92] Data 0.551 (0.551) Elapsed 0m 0s (remain 1m 19s) Loss: 0.2175(0.2175) Grad: 0.3218  \n","Epoch: [9][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2231(0.2197) Grad: 0.3022  \n","EVAL: [0/16] Data 0.652 (0.652) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2299(0.2299) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.2197  avg_val_loss: 0.2100  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2012(0.2100) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - Save Best Loss: 0.2100 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [10][0/92] Data 0.578 (0.578) Elapsed 0m 0s (remain 1m 24s) Loss: 0.2163(0.2163) Grad: 0.3981  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2146(0.2176) Grad: 0.3588  \n","EVAL: [0/16] Data 0.532 (0.532) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2211(0.2211) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.2176  avg_val_loss: 0.2067  time: 23s\n","Epoch 10 - Save Best Loss: 0.2067 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1981(0.2067) \n","Epoch: [11][0/92] Data 0.471 (0.471) Elapsed 0m 0s (remain 1m 9s) Loss: 0.2059(0.2059) Grad: 0.3626  \n","Epoch: [11][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2002(0.2154) Grad: 0.2842  \n","EVAL: [0/16] Data 0.597 (0.597) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2108(0.2108) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.2154  avg_val_loss: 0.2038  time: 22s\n","Epoch 11 - Save Best Loss: 0.2038 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1906(0.2038) \n","Epoch: [12][0/92] Data 0.468 (0.468) Elapsed 0m 0s (remain 1m 10s) Loss: 0.1944(0.1944) Grad: 0.2954  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1910(0.2139) Grad: 0.2507  \n","EVAL: [0/16] Data 0.687 (0.687) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2200(0.2200) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.2139  avg_val_loss: 0.2051  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1864(0.2051) \n","Epoch: [13][0/92] Data 0.518 (0.518) Elapsed 0m 0s (remain 1m 18s) Loss: 0.1958(0.1958) Grad: 0.7273  \n","Epoch: [13][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1958(0.2136) Grad: 0.2737  \n","EVAL: [0/16] Data 0.540 (0.540) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2185(0.2185) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.2136  avg_val_loss: 0.2003  time: 22s\n","Epoch 13 - Save Best Loss: 0.2003 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1813(0.2003) \n","Epoch: [14][0/92] Data 0.585 (0.585) Elapsed 0m 0s (remain 1m 21s) Loss: 0.2112(0.2112) Grad: 0.2890  \n","Epoch: [14][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1698(0.2097) Grad: 0.7836  \n","EVAL: [0/16] Data 0.648 (0.648) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2071(0.2071) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.2097  avg_val_loss: 0.1961  time: 25s\n","Epoch 14 - Save Best Loss: 0.1961 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1804(0.1961) \n","Epoch: [15][0/92] Data 0.671 (0.671) Elapsed 0m 0s (remain 1m 25s) Loss: 0.1893(0.1893) Grad: 0.2241  \n","Epoch: [15][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2334(0.2084) Grad: 0.2854  \n","EVAL: [0/16] Data 0.657 (0.657) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2021(0.2021) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.2084  avg_val_loss: 0.1973  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1797(0.1973) \n","Epoch: [16][0/92] Data 0.486 (0.486) Elapsed 0m 0s (remain 1m 11s) Loss: 0.1951(0.1951) Grad: 0.3290  \n","Epoch: [16][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2020(0.2089) Grad: 0.2795  \n","EVAL: [0/16] Data 0.503 (0.503) Elapsed 0m 0s (remain 0m 8s) Loss: 0.1982(0.1982) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.2089  avg_val_loss: 0.1933  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1796(0.1933) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - Save Best Loss: 0.1933 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [17][0/92] Data 0.491 (0.491) Elapsed 0m 0s (remain 1m 19s) Loss: 0.2152(0.2152) Grad: 0.3023  \n","Epoch: [17][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1917(0.2066) Grad: 0.4646  \n","EVAL: [0/16] Data 0.743 (0.743) Elapsed 0m 0s (remain 0m 12s) Loss: 0.1977(0.1977) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.2066  avg_val_loss: 0.1904  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.081) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1767(0.1904) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - Save Best Loss: 0.1904 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [18][0/92] Data 0.497 (0.497) Elapsed 0m 0s (remain 1m 23s) Loss: 0.1940(0.1940) Grad: 0.3115  \n","Epoch: [18][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1919(0.2036) Grad: 1.7347  \n","EVAL: [0/16] Data 0.718 (0.718) Elapsed 0m 0s (remain 0m 12s) Loss: 0.1946(0.1946) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.2036  avg_val_loss: 0.1901  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1752(0.1901) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - Save Best Loss: 0.1901 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [19][0/92] Data 0.392 (0.392) Elapsed 0m 0s (remain 1m 4s) Loss: 0.2007(0.2007) Grad: 0.5122  \n","Epoch: [19][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2389(0.2051) Grad: 0.3130  \n","EVAL: [0/16] Data 0.646 (0.646) Elapsed 0m 0s (remain 0m 11s) Loss: 0.1981(0.1981) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.2051  avg_val_loss: 0.1895  time: 22s\n","Epoch 19 - Save Best Loss: 0.1895 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1775(0.1895) \n","Epoch: [20][0/92] Data 0.605 (0.605) Elapsed 0m 0s (remain 1m 19s) Loss: 0.2037(0.2037) Grad: 0.2819  \n","Epoch: [20][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1973(0.2028) Grad: 0.4924  \n","EVAL: [0/16] Data 0.585 (0.585) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1966(0.1966) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.2028  avg_val_loss: 0.1893  time: 22s\n","Epoch 20 - Save Best Loss: 0.1893 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1745(0.1893) \n","Epoch: [21][0/92] Data 0.617 (0.617) Elapsed 0m 0s (remain 1m 27s) Loss: 0.1959(0.1959) Grad: 0.3525  \n","Epoch: [21][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2654(0.2021) Grad: 0.5320  \n","EVAL: [0/16] Data 0.639 (0.639) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1949(0.1949) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.2021  avg_val_loss: 0.1895  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1766(0.1895) \n","Epoch: [22][0/92] Data 0.488 (0.488) Elapsed 0m 0s (remain 1m 14s) Loss: 0.1665(0.1665) Grad: 0.2612  \n","Epoch: [22][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1612(0.2014) Grad: 0.2730  \n","EVAL: [0/16] Data 0.633 (0.633) Elapsed 0m 0s (remain 0m 10s) Loss: 0.1949(0.1949) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.2014  avg_val_loss: 0.1893  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1764(0.1893) \n","Epoch: [23][0/92] Data 0.574 (0.574) Elapsed 0m 1s (remain 1m 35s) Loss: 0.2364(0.2364) Grad: 0.4193  \n","Epoch: [23][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1863(0.2000) Grad: 0.6351  \n","EVAL: [0/16] Data 0.554 (0.554) Elapsed 0m 0s (remain 0m 9s) Loss: 0.1967(0.1967) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.2000  avg_val_loss: 0.1898  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1783(0.1898) \n","Epoch: [24][0/92] Data 0.493 (0.493) Elapsed 0m 0s (remain 1m 16s) Loss: 0.2055(0.2055) Grad: 0.2632  \n","Epoch: [24][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1884(0.2014) Grad: 0.4860  \n","EVAL: [0/16] Data 0.793 (0.793) Elapsed 0m 0s (remain 0m 13s) Loss: 0.1955(0.1955) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.2014  avg_val_loss: 0.1899  time: 24s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1770(0.1899) \n","Epoch: [25][0/92] Data 0.406 (0.406) Elapsed 0m 0s (remain 1m 7s) Loss: 0.1844(0.1844) Grad: 0.2984  \n","Epoch: [25][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2484(0.2000) Grad: 0.5531  \n","EVAL: [0/16] Data 0.810 (0.810) Elapsed 0m 0s (remain 0m 13s) Loss: 0.1974(0.1974) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.2000  avg_val_loss: 0.1904  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.083) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1796(0.1904) \n"],"name":"stdout"},{"output_type":"stream","text":["========== fold: 2 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.503 (0.503) Elapsed 0m 0s (remain 1m 14s) Loss: 4.6493(4.6493) Grad: 26.6529  \n","Epoch: [1][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 4.3822(4.2141) Grad: 24.3493  \n","EVAL: [0/16] Data 0.660 (0.660) Elapsed 0m 0s (remain 0m 11s) Loss: 4.0888(4.0888) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 4.2141  avg_val_loss: 3.8811  time: 22s\n","Epoch 1 - Score: 1.9754\n","Epoch 1 - Save Best Loss: 3.8811 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 4.7350(3.8811) \n","Epoch: [2][0/92] Data 0.565 (0.565) Elapsed 0m 0s (remain 1m 20s) Loss: 4.7136(4.7136) Grad: 25.9597  \n","Epoch: [2][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1463(1.9323) Grad: 4.7251  \n","EVAL: [0/16] Data 0.788 (0.788) Elapsed 0m 0s (remain 0m 13s) Loss: 1.1022(1.1022) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 1.9323  avg_val_loss: 1.0875  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.4376(1.0875) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - Score: 1.0355\n","Epoch 2 - Save Best Loss: 1.0875 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [3][0/92] Data 0.577 (0.577) Elapsed 0m 0s (remain 1m 30s) Loss: 0.5909(0.5909) Grad: 1.2515  \n","Epoch: [3][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0417(0.9758) Grad: 4.7943  \n","EVAL: [0/16] Data 0.709 (0.709) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8977(0.8977) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.9758  avg_val_loss: 0.9449  time: 22s\n","Epoch 3 - Score: 0.9639\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1766(0.9449) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - Save Best Loss: 0.9449 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [4][0/92] Data 0.532 (0.532) Elapsed 0m 0s (remain 1m 16s) Loss: 0.9964(0.9964) Grad: 1.7862  \n","Epoch: [4][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7019(0.9580) Grad: 2.9723  \n","EVAL: [0/16] Data 0.635 (0.635) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8948(0.8948) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.9580  avg_val_loss: 0.9547  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1726(0.9547) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - Score: 0.9719\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [5][0/92] Data 0.601 (0.601) Elapsed 0m 0s (remain 1m 20s) Loss: 0.9170(0.9170) Grad: 0.8004  \n","Epoch: [5][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8969(0.9523) Grad: 1.4308  \n","EVAL: [0/16] Data 0.848 (0.848) Elapsed 0m 0s (remain 0m 14s) Loss: 0.8939(0.8939) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.9523  avg_val_loss: 0.9452  time: 22s\n","Epoch 5 - Score: 0.9647\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1670(0.9452) \n","Epoch: [6][0/92] Data 0.427 (0.427) Elapsed 0m 0s (remain 1m 8s) Loss: 1.1010(1.1010) Grad: 1.5913  \n","Epoch: [6][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 1.4391(0.9532) Grad: 4.8338  \n","EVAL: [0/16] Data 0.674 (0.674) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8816(0.8816) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.9532  avg_val_loss: 0.9427  time: 22s\n","Epoch 6 - Score: 0.9690\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1459(0.9427) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - Save Best Loss: 0.9427 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [7][0/92] Data 0.676 (0.676) Elapsed 0m 0s (remain 1m 24s) Loss: 0.8274(0.8274) Grad: 0.3220  \n","Epoch: [7][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8860(0.9491) Grad: 1.2857  \n","EVAL: [0/16] Data 0.735 (0.735) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9118(0.9118) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.9491  avg_val_loss: 0.9659  time: 22s\n","Epoch 7 - Score: 0.9689\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1998(0.9659) \n","Epoch: [8][0/92] Data 0.539 (0.539) Elapsed 0m 0s (remain 1m 21s) Loss: 1.3058(1.3058) Grad: 1.3109  \n","Epoch: [8][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 1.0905(0.9494) Grad: 1.8882  \n","EVAL: [0/16] Data 0.642 (0.642) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8880(0.8880) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.9494  avg_val_loss: 0.9397  time: 22s\n","Epoch 8 - Score: 0.9723\n","Epoch 8 - Save Best Loss: 0.9397 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1633(0.9397) \n","Epoch: [9][0/92] Data 0.436 (0.436) Elapsed 0m 0s (remain 1m 8s) Loss: 0.6183(0.6183) Grad: 0.6162  \n","Epoch: [9][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8504(0.9511) Grad: 2.3310  \n","EVAL: [0/16] Data 0.704 (0.704) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8881(0.8881) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.9511  avg_val_loss: 0.9432  time: 23s\n","Epoch 9 - Score: 0.9679\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1631(0.9432) \n","Epoch: [10][0/92] Data 0.581 (0.581) Elapsed 0m 0s (remain 1m 22s) Loss: 1.0818(1.0818) Grad: 1.1749  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.5951(0.9468) Grad: 0.1986  \n","EVAL: [0/16] Data 0.734 (0.734) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9269(0.9269) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.9468  avg_val_loss: 1.0634  time: 23s\n","Epoch 10 - Score: 1.0185\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1496(1.0634) \n","Epoch: [11][0/92] Data 0.476 (0.476) Elapsed 0m 0s (remain 1m 13s) Loss: 0.9886(0.9886) Grad: 1.6151  \n","Epoch: [11][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 1.1043(0.9506) Grad: 3.5614  \n","EVAL: [0/16] Data 0.774 (0.774) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8848(0.8848) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.9506  avg_val_loss: 0.9399  time: 22s\n","Epoch 11 - Score: 0.9696\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1528(0.9399) \n","Epoch: [12][0/92] Data 0.498 (0.498) Elapsed 0m 0s (remain 1m 14s) Loss: 0.8264(0.8264) Grad: 0.2589  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8101(0.9448) Grad: 1.6887  \n","EVAL: [0/16] Data 0.660 (0.660) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8814(0.8814) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.9448  avg_val_loss: 0.9425  time: 22s\n","Epoch 12 - Score: 0.9695\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1448(0.9425) \n","Epoch: [13][0/92] Data 0.429 (0.429) Elapsed 0m 0s (remain 1m 9s) Loss: 0.8600(0.8600) Grad: 1.9134  \n","Epoch: [13][91/92] Data 0.000 (0.005) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8220(0.9478) Grad: 0.5168  \n","EVAL: [0/16] Data 0.597 (0.597) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8879(0.8879) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.9478  avg_val_loss: 0.9456  time: 23s\n","Epoch 13 - Score: 0.9683\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1614(0.9456) \n","Epoch: [14][0/92] Data 0.604 (0.604) Elapsed 0m 0s (remain 1m 25s) Loss: 1.2718(1.2718) Grad: 5.4920  \n","Epoch: [14][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.9418(0.9451) Grad: 0.1915  \n","EVAL: [0/16] Data 0.653 (0.653) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9102(0.9102) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.9451  avg_val_loss: 0.9490  time: 23s\n","Epoch 14 - Score: 0.9714\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2002(0.9490) \n","Epoch: [15][0/92] Data 0.458 (0.458) Elapsed 0m 0s (remain 1m 11s) Loss: 0.9007(0.9007) Grad: 2.3161  \n","Epoch: [15][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8571(0.9443) Grad: 1.0721  \n","EVAL: [0/16] Data 0.644 (0.644) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8810(0.8810) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.9443  avg_val_loss: 0.9420  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1434(0.9420) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - Score: 0.9665\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [16][0/92] Data 0.533 (0.533) Elapsed 0m 0s (remain 1m 17s) Loss: 1.2239(1.2239) Grad: 0.6952  \n","Epoch: [16][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0577(0.9445) Grad: 0.8312  \n","EVAL: [0/16] Data 0.738 (0.738) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8829(0.8829) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.9445  avg_val_loss: 0.9404  time: 22s\n","Epoch 16 - Score: 0.9672\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1509(0.9404) \n","Epoch: [17][0/92] Data 0.641 (0.641) Elapsed 0m 0s (remain 1m 22s) Loss: 0.7395(0.7395) Grad: 15.2391  \n","Epoch: [17][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6999(0.9459) Grad: 0.6848  \n","EVAL: [0/16] Data 0.723 (0.723) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8838(0.8838) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.9459  avg_val_loss: 0.9395  time: 22s\n","Epoch 17 - Score: 0.9658\n","Epoch 17 - Save Best Loss: 0.9395 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1528(0.9395) \n","Epoch: [18][0/92] Data 0.629 (0.629) Elapsed 0m 0s (remain 1m 26s) Loss: 0.8688(0.8688) Grad: 0.6181  \n","Epoch: [18][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7719(0.9434) Grad: 1.3475  \n","EVAL: [0/16] Data 0.736 (0.736) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8896(0.8896) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.9434  avg_val_loss: 0.9390  time: 22s\n","Epoch 18 - Score: 0.9684\n","Epoch 18 - Save Best Loss: 0.9390 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1650(0.9390) \n","Epoch: [19][0/92] Data 0.500 (0.500) Elapsed 0m 0s (remain 1m 14s) Loss: 1.1453(1.1453) Grad: 1.9640  \n","Epoch: [19][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9173(0.9461) Grad: 2.6982  \n","EVAL: [0/16] Data 0.766 (0.766) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8867(0.8867) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.9461  avg_val_loss: 0.9390  time: 22s\n","Epoch 19 - Score: 0.9665\n","Epoch 19 - Save Best Loss: 0.9390 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1588(0.9390) \n","Epoch: [20][0/92] Data 0.520 (0.520) Elapsed 0m 0s (remain 1m 15s) Loss: 0.9747(0.9747) Grad: 2.2402  \n","Epoch: [20][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.5240(0.9459) Grad: 0.8300  \n","EVAL: [0/16] Data 0.559 (0.559) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8879(0.8879) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.9459  avg_val_loss: 0.9388  time: 23s\n","Epoch 20 - Score: 0.9690\n","Epoch 20 - Save Best Loss: 0.9388 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1626(0.9388) \n","Epoch: [21][0/92] Data 0.618 (0.618) Elapsed 0m 0s (remain 1m 22s) Loss: 0.8967(0.8967) Grad: 1.4987  \n","Epoch: [21][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.5949(0.9458) Grad: 0.1955  \n","EVAL: [0/16] Data 0.621 (0.621) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8860(0.8860) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.9458  avg_val_loss: 0.9387  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.078) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1576(0.9387) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - Score: 0.9680\n","Epoch 21 - Save Best Loss: 0.9387 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [22][0/92] Data 0.526 (0.526) Elapsed 0m 0s (remain 1m 14s) Loss: 1.4515(1.4515) Grad: 2.9355  \n","Epoch: [22][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 1.0944(0.9452) Grad: 2.3308  \n","EVAL: [0/16] Data 0.467 (0.467) Elapsed 0m 0s (remain 0m 8s) Loss: 0.8874(0.8874) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.9452  avg_val_loss: 0.9387  time: 22s\n","Epoch 22 - Score: 0.9675\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1606(0.9387) \n","Epoch: [23][0/92] Data 0.551 (0.551) Elapsed 0m 0s (remain 1m 23s) Loss: 0.8356(0.8356) Grad: 2.7996  \n","Epoch: [23][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8248(0.9437) Grad: 0.6466  \n","EVAL: [0/16] Data 0.637 (0.637) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8876(0.8876) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.9437  avg_val_loss: 0.9393  time: 22s\n","Epoch 23 - Score: 0.9675\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1608(0.9393) \n","Epoch: [24][0/92] Data 0.633 (0.633) Elapsed 0m 0s (remain 1m 27s) Loss: 0.8905(0.8905) Grad: 0.6438  \n","Epoch: [24][91/92] Data 0.000 (0.007) Elapsed 0m 18s (remain 0m 0s) Loss: 0.9430(0.9427) Grad: 2.8462  \n","EVAL: [0/16] Data 0.664 (0.664) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8874(0.8874) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.9427  avg_val_loss: 0.9387  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1611(0.9387) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - Score: 0.9680\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [25][0/92] Data 0.514 (0.514) Elapsed 0m 0s (remain 1m 19s) Loss: 0.8198(0.8198) Grad: 0.6136  \n","Epoch: [25][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.8887(0.9436) Grad: 0.5620  \n","EVAL: [0/16] Data 0.631 (0.631) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8881(0.8881) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.9436  avg_val_loss: 0.9386  time: 22s\n","Epoch 25 - Score: 0.9680\n","Epoch 25 - Save Best Loss: 0.9386 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 1.1624(0.9386) \n"],"name":"stdout"},{"output_type":"stream","text":["========== fold: 2 result ==========\n","Score: 0.9680\n","========== fold 3 materials ================\n","========== fold: 3 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.619 (0.619) Elapsed 0m 0s (remain 1m 27s) Loss: 0.6988(0.6988) Grad: 0.8554  \n","Epoch: [1][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6971(0.6997) Grad: 1.0052  \n","EVAL: [0/16] Data 0.620 (0.620) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8709(0.8709) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.6997  avg_val_loss: 0.8073  time: 22s\n","Epoch 1 - Save Best Loss: 0.8073 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.6928(0.8073) \n","Epoch: [2][0/92] Data 0.497 (0.497) Elapsed 0m 0s (remain 1m 9s) Loss: 0.6962(0.6962) Grad: 0.8691  \n","Epoch: [2][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.6633(0.6856) Grad: 1.0289  \n","EVAL: [0/16] Data 0.727 (0.727) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8837(0.8837) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.6856  avg_val_loss: 0.8133  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.6559(0.8133) \n","Epoch: [3][0/92] Data 0.498 (0.498) Elapsed 0m 0s (remain 1m 14s) Loss: 0.6688(0.6688) Grad: 0.8965  \n","Epoch: [3][91/92] Data 0.000 (0.006) Elapsed 0m 18s (remain 0m 0s) Loss: 0.4356(0.5857) Grad: 0.7256  \n","EVAL: [0/16] Data 0.779 (0.779) Elapsed 0m 0s (remain 0m 12s) Loss: 0.7669(0.7669) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5857  avg_val_loss: 0.6753  time: 22s\n","Epoch 3 - Save Best Loss: 0.6753 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.4500(0.6753) \n","Epoch: [4][0/92] Data 0.649 (0.649) Elapsed 0m 0s (remain 1m 30s) Loss: 0.4481(0.4481) Grad: 0.9406  \n","Epoch: [4][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2243(0.2999) Grad: 0.3835  \n","EVAL: [0/16] Data 0.644 (0.644) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2923(0.2923) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.2999  avg_val_loss: 0.2618  time: 23s\n","Epoch 4 - Save Best Loss: 0.2618 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2511(0.2618) \n","Epoch: [5][0/92] Data 0.524 (0.524) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2413(0.2413) Grad: 0.2851  \n","Epoch: [5][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2315(0.2349) Grad: 0.2508  \n","EVAL: [0/16] Data 0.563 (0.563) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2609(0.2609) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.2349  avg_val_loss: 0.2386  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2341(0.2386) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - Save Best Loss: 0.2386 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [6][0/92] Data 0.509 (0.509) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2337(0.2337) Grad: 0.5523  \n","Epoch: [6][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2397(0.2295) Grad: 1.6691  \n","EVAL: [0/16] Data 0.793 (0.793) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2776(0.2776) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.2295  avg_val_loss: 0.2387  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2246(0.2387) \n","Epoch: [7][0/92] Data 0.549 (0.549) Elapsed 0m 0s (remain 1m 24s) Loss: 0.2302(0.2302) Grad: 0.2762  \n","Epoch: [7][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2148(0.2253) Grad: 0.2755  \n","EVAL: [0/16] Data 0.620 (0.620) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2691(0.2691) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.2253  avg_val_loss: 0.2346  time: 23s\n","Epoch 7 - Save Best Loss: 0.2346 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2276(0.2346) \n","Epoch: [8][0/92] Data 0.562 (0.562) Elapsed 0m 0s (remain 1m 25s) Loss: 0.2504(0.2504) Grad: 0.3443  \n","Epoch: [8][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2242(0.2216) Grad: 0.3182  \n","EVAL: [0/16] Data 0.784 (0.784) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2453(0.2453) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.2216  avg_val_loss: 0.2226  time: 22s\n","Epoch 8 - Save Best Loss: 0.2226 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.075) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2226(0.2226) \n","Epoch: [9][0/92] Data 0.676 (0.676) Elapsed 0m 0s (remain 1m 26s) Loss: 0.2260(0.2260) Grad: 0.2973  \n","Epoch: [9][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2072(0.2200) Grad: 0.4956  \n","EVAL: [0/16] Data 0.698 (0.698) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2419(0.2419) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.2200  avg_val_loss: 0.2181  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.085) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2167(0.2181) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - Save Best Loss: 0.2181 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [10][0/92] Data 0.590 (0.590) Elapsed 0m 0s (remain 1m 27s) Loss: 0.1974(0.1974) Grad: 0.3056  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1934(0.2161) Grad: 0.4894  \n","EVAL: [0/16] Data 0.747 (0.747) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2419(0.2419) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.2161  avg_val_loss: 0.2192  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2095(0.2192) \n","Epoch: [11][0/92] Data 0.607 (0.607) Elapsed 0m 0s (remain 1m 21s) Loss: 0.2016(0.2016) Grad: 0.4975  \n","Epoch: [11][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2276(0.2141) Grad: 0.6018  \n","EVAL: [0/16] Data 0.485 (0.485) Elapsed 0m 0s (remain 0m 8s) Loss: 0.2371(0.2371) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.2141  avg_val_loss: 0.2145  time: 22s\n","Epoch 11 - Save Best Loss: 0.2145 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1998(0.2145) \n","Epoch: [12][0/92] Data 0.466 (0.466) Elapsed 0m 0s (remain 1m 10s) Loss: 0.2621(0.2621) Grad: 0.4336  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1954(0.2136) Grad: 0.3548  \n","EVAL: [0/16] Data 0.589 (0.589) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2461(0.2461) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.2136  avg_val_loss: 0.2158  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2007(0.2158) \n","Epoch: [13][0/92] Data 0.444 (0.444) Elapsed 0m 0s (remain 1m 14s) Loss: 0.2343(0.2343) Grad: 0.4545  \n","Epoch: [13][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2329(0.2102) Grad: 0.3342  \n","EVAL: [0/16] Data 0.715 (0.715) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2315(0.2315) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.2102  avg_val_loss: 0.2109  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2063(0.2109) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - Save Best Loss: 0.2109 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [14][0/92] Data 0.538 (0.538) Elapsed 0m 0s (remain 1m 20s) Loss: 0.2381(0.2381) Grad: 0.4108  \n","Epoch: [14][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2684(0.2087) Grad: 0.3686  \n","EVAL: [0/16] Data 0.697 (0.697) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2344(0.2344) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.2087  avg_val_loss: 0.2079  time: 26s\n","Epoch 14 - Save Best Loss: 0.2079 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2052(0.2079) \n","Epoch: [15][0/92] Data 0.674 (0.674) Elapsed 0m 0s (remain 1m 26s) Loss: 0.2132(0.2132) Grad: 0.5408  \n","Epoch: [15][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2119(0.2071) Grad: 0.2658  \n","EVAL: [0/16] Data 0.789 (0.789) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2335(0.2335) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.2071  avg_val_loss: 0.2080  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.071) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1993(0.2080) \n","Epoch: [16][0/92] Data 0.602 (0.602) Elapsed 0m 0s (remain 1m 22s) Loss: 0.2041(0.2041) Grad: 0.4026  \n","Epoch: [16][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2055(0.2070) Grad: 0.3357  \n","EVAL: [0/16] Data 0.681 (0.681) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2323(0.2323) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.2070  avg_val_loss: 0.2081  time: 24s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.065) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2037(0.2081) \n","Epoch: [17][0/92] Data 0.496 (0.496) Elapsed 0m 0s (remain 1m 17s) Loss: 0.2096(0.2096) Grad: 0.3065  \n","Epoch: [17][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1837(0.2059) Grad: 0.4688  \n","EVAL: [0/16] Data 0.810 (0.810) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2248(0.2248) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.2059  avg_val_loss: 0.2059  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.080) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1990(0.2059) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - Save Best Loss: 0.2059 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [18][0/92] Data 0.551 (0.551) Elapsed 0m 0s (remain 1m 26s) Loss: 0.2121(0.2121) Grad: 0.4424  \n","Epoch: [18][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.1876(0.2043) Grad: 0.4794  \n","EVAL: [0/16] Data 0.851 (0.851) Elapsed 0m 0s (remain 0m 14s) Loss: 0.2326(0.2326) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.2043  avg_val_loss: 0.2087  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.085) Elapsed 0m 2s (remain 0m 0s) Loss: 0.2020(0.2087) \n","Epoch: [19][0/92] Data 0.575 (0.575) Elapsed 0m 0s (remain 1m 18s) Loss: 0.1564(0.1564) Grad: 0.3634  \n","Epoch: [19][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2510(0.2027) Grad: 0.5878  \n","EVAL: [0/16] Data 0.611 (0.611) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2345(0.2345) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.2027  avg_val_loss: 0.2090  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.081) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1991(0.2090) \n","Epoch: [20][0/92] Data 0.524 (0.524) Elapsed 0m 0s (remain 1m 16s) Loss: 0.2494(0.2494) Grad: 0.3751  \n","Epoch: [20][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2005(0.2030) Grad: 0.4578  \n","EVAL: [0/16] Data 0.531 (0.531) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2318(0.2318) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.2030  avg_val_loss: 0.2064  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1967(0.2064) \n","Epoch: [21][0/92] Data 0.500 (0.500) Elapsed 0m 0s (remain 1m 12s) Loss: 0.1794(0.1794) Grad: 0.4344  \n","Epoch: [21][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2016(0.2010) Grad: 0.6867  \n","EVAL: [0/16] Data 0.703 (0.703) Elapsed 0m 0s (remain 0m 12s) Loss: 0.2255(0.2255) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.2010  avg_val_loss: 0.2036  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1956(0.2036) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - Save Best Loss: 0.2036 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [22][0/92] Data 0.443 (0.443) Elapsed 0m 0s (remain 1m 15s) Loss: 0.2424(0.2424) Grad: 0.4357  \n","Epoch: [22][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2157(0.2022) Grad: 0.3752  \n","EVAL: [0/16] Data 0.534 (0.534) Elapsed 0m 0s (remain 0m 9s) Loss: 0.2273(0.2273) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.2022  avg_val_loss: 0.2059  time: 24s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1961(0.2059) \n","Epoch: [23][0/92] Data 0.476 (0.476) Elapsed 0m 0s (remain 1m 17s) Loss: 0.2326(0.2326) Grad: 0.3869  \n","Epoch: [23][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2134(0.2008) Grad: 0.4722  \n","EVAL: [0/16] Data 0.706 (0.706) Elapsed 0m 0s (remain 0m 11s) Loss: 0.2273(0.2273) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.2008  avg_val_loss: 0.2056  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1964(0.2056) \n","Epoch: [24][0/92] Data 0.538 (0.538) Elapsed 0m 0s (remain 1m 21s) Loss: 0.1824(0.1824) Grad: 0.3393  \n","Epoch: [24][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2001(0.2010) Grad: 0.4641  \n","EVAL: [0/16] Data 0.793 (0.793) Elapsed 0m 0s (remain 0m 13s) Loss: 0.2315(0.2315) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.2010  avg_val_loss: 0.2060  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.077) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1971(0.2060) \n","Epoch: [25][0/92] Data 0.602 (0.602) Elapsed 0m 0s (remain 1m 29s) Loss: 0.1909(0.1909) Grad: 0.5925  \n","Epoch: [25][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.2305(0.2014) Grad: 0.5325  \n","EVAL: [0/16] Data 0.560 (0.560) Elapsed 0m 0s (remain 0m 10s) Loss: 0.2265(0.2265) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.2014  avg_val_loss: 0.2058  time: 24s\n","========== fold: 3 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.064) Elapsed 0m 2s (remain 0m 0s) Loss: 0.1959(0.2058) \n","Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n","Epoch: [1][0/92] Data 0.416 (0.416) Elapsed 0m 0s (remain 1m 7s) Loss: 4.4514(4.4514) Grad: 24.8341  \n","Epoch: [1][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 3.9113(3.9463) Grad: 24.0182  \n","EVAL: [0/16] Data 0.709 (0.709) Elapsed 0m 0s (remain 0m 12s) Loss: 3.4385(3.4385) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1 - avg_train_loss: 3.9463  avg_val_loss: 3.5448  time: 22s\n","Epoch 1 - Score: 1.8818\n","Epoch 1 - Save Best Loss: 3.5448 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.074) Elapsed 0m 2s (remain 0m 0s) Loss: 2.1893(3.5448) \n","Epoch: [2][0/92] Data 0.502 (0.502) Elapsed 0m 0s (remain 1m 15s) Loss: 3.8484(3.8484) Grad: 23.5904  \n","Epoch: [2][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.3146(1.7976) Grad: 3.8887  \n","EVAL: [0/16] Data 0.712 (0.712) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9329(0.9329) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - avg_train_loss: 1.7976  avg_val_loss: 1.0829  time: 26s\n","Epoch 2 - Score: 1.0375\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.0744(1.0829) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2 - Save Best Loss: 1.0829 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [3][0/92] Data 0.486 (0.486) Elapsed 0m 0s (remain 1m 17s) Loss: 1.1874(1.1874) Grad: 4.6679  \n","Epoch: [3][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.3504(0.9663) Grad: 0.8342  \n","EVAL: [0/16] Data 0.590 (0.590) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8877(0.8877) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.9663  avg_val_loss: 1.0099  time: 23s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2479(1.0099) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3 - Score: 0.9958\n","Epoch 3 - Save Best Loss: 1.0099 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [4][0/92] Data 0.590 (0.590) Elapsed 0m 0s (remain 1m 26s) Loss: 0.9169(0.9169) Grad: 2.0440  \n","Epoch: [4][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8395(0.9532) Grad: 1.8475  \n","EVAL: [0/16] Data 0.767 (0.767) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9119(0.9119) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.9532  avg_val_loss: 0.9912  time: 22s\n","Epoch 4 - Score: 0.9912\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3165(0.9912) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 4 - Save Best Loss: 0.9912 Model\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [5][0/92] Data 0.618 (0.618) Elapsed 0m 0s (remain 1m 22s) Loss: 0.7467(0.7467) Grad: 2.1399  \n","Epoch: [5][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8146(0.9500) Grad: 1.1405  \n","EVAL: [0/16] Data 0.716 (0.716) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8205(0.8205) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.9500  avg_val_loss: 0.9478  time: 28s\n","Epoch 5 - Score: 0.9719\n","Epoch 5 - Save Best Loss: 0.9478 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3226(0.9478) \n","Epoch: [6][0/92] Data 0.544 (0.544) Elapsed 0m 0s (remain 1m 18s) Loss: 1.1086(1.1086) Grad: 1.2302  \n","Epoch: [6][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9149(0.9494) Grad: 2.1086  \n","EVAL: [0/16] Data 0.739 (0.739) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8214(0.8214) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.9494  avg_val_loss: 0.9477  time: 22s\n","Epoch 6 - Score: 0.9730\n","Epoch 6 - Save Best Loss: 0.9477 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3056(0.9477) \n","Epoch: [7][0/92] Data 0.617 (0.617) Elapsed 0m 0s (remain 1m 24s) Loss: 0.6476(0.6476) Grad: 0.5922  \n","Epoch: [7][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1469(0.9469) Grad: 1.3557  \n","EVAL: [0/16] Data 0.471 (0.471) Elapsed 0m 0s (remain 0m 8s) Loss: 0.8206(0.8206) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.9469  avg_val_loss: 0.9461  time: 25s\n","Epoch 7 - Score: 0.9672\n","Epoch 7 - Save Best Loss: 0.9461 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.061) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2606(0.9461) \n","Epoch: [8][0/92] Data 0.602 (0.602) Elapsed 0m 0s (remain 1m 23s) Loss: 0.8981(0.8981) Grad: 2.0932  \n","Epoch: [8][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7834(0.9480) Grad: 2.1818  \n","EVAL: [0/16] Data 0.563 (0.563) Elapsed 0m 0s (remain 0m 9s) Loss: 0.8229(0.8229) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.9480  avg_val_loss: 0.9459  time: 23s\n","Epoch 8 - Score: 0.9698\n","Epoch 8 - Save Best Loss: 0.9459 Model\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2440(0.9459) \n","Epoch: [9][0/92] Data 0.514 (0.514) Elapsed 0m 0s (remain 1m 18s) Loss: 0.8610(0.8610) Grad: 1.5963  \n","Epoch: [9][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2105(0.9499) Grad: 1.2329  \n","EVAL: [0/16] Data 0.718 (0.718) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8814(0.8814) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.9499  avg_val_loss: 0.9851  time: 23s\n","Epoch 9 - Score: 0.9907\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.070) Elapsed 0m 2s (remain 0m 0s) Loss: 1.4465(0.9851) \n","Epoch: [10][0/92] Data 0.550 (0.550) Elapsed 0m 0s (remain 1m 19s) Loss: 1.0501(1.0501) Grad: 1.6965  \n","Epoch: [10][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9461(0.9511) Grad: 1.0581  \n","EVAL: [0/16] Data 0.623 (0.623) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8380(0.8380) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.9511  avg_val_loss: 0.9519  time: 26s\n","Epoch 10 - Score: 0.9741\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2975(0.9519) \n","Epoch: [11][0/92] Data 0.549 (0.549) Elapsed 0m 0s (remain 1m 19s) Loss: 0.8584(0.8584) Grad: 3.6335  \n","Epoch: [11][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8330(0.9483) Grad: 0.8586  \n","EVAL: [0/16] Data 0.864 (0.864) Elapsed 0m 0s (remain 0m 14s) Loss: 1.0261(1.0261) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.9483  avg_val_loss: 1.0418  time: 24s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3276(1.0418) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 11 - Score: 1.0205\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [12][0/92] Data 0.478 (0.478) Elapsed 0m 0s (remain 1m 19s) Loss: 0.8342(0.8342) Grad: 3.2081  \n","Epoch: [12][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.8023(0.9446) Grad: 2.6234  \n","EVAL: [0/16] Data 0.827 (0.827) Elapsed 0m 0s (remain 0m 14s) Loss: 0.8313(0.8313) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.9446  avg_val_loss: 0.9487  time: 23s\n","Epoch 12 - Score: 0.9731\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3543(0.9487) \n","Epoch: [13][0/92] Data 0.425 (0.425) Elapsed 0m 0s (remain 1m 11s) Loss: 0.8512(0.8512) Grad: 1.4235  \n","Epoch: [13][91/92] Data 0.000 (0.005) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0887(0.9467) Grad: 3.5059  \n","EVAL: [0/16] Data 0.787 (0.787) Elapsed 0m 0s (remain 0m 13s) Loss: 0.8344(0.8344) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.9467  avg_val_loss: 0.9498  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3360(0.9498) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 13 - Score: 0.9749\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [14][0/92] Data 0.451 (0.451) Elapsed 0m 0s (remain 1m 14s) Loss: 1.0156(1.0156) Grad: 0.3665  \n","Epoch: [14][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1155(0.9443) Grad: 1.5522  \n","EVAL: [0/16] Data 0.525 (0.525) Elapsed 0m 0s (remain 0m 9s) Loss: 0.8323(0.8323) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.9443  avg_val_loss: 0.9850  time: 22s\n","Epoch 14 - Score: 0.9892\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3280(0.9850) \n","Epoch: [15][0/92] Data 0.646 (0.646) Elapsed 0m 0s (remain 1m 22s) Loss: 0.9798(0.9798) Grad: 0.9563  \n","Epoch: [15][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0738(0.9465) Grad: 1.2287  \n","EVAL: [0/16] Data 0.638 (0.638) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8194(0.8194) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.9465  avg_val_loss: 1.0084  time: 26s\n","Epoch 15 - Score: 1.0030\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.069) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3290(1.0084) \n","Epoch: [16][0/92] Data 0.595 (0.595) Elapsed 0m 0s (remain 1m 22s) Loss: 0.8487(0.8487) Grad: 1.0831  \n","Epoch: [16][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1653(0.9428) Grad: 2.2540  \n","EVAL: [0/16] Data 0.711 (0.711) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8265(0.8265) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.9428  avg_val_loss: 1.1687  time: 25s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2071(1.1687) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 16 - Score: 1.0607\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [17][0/92] Data 0.581 (0.581) Elapsed 0m 0s (remain 1m 24s) Loss: 0.8655(0.8655) Grad: 0.4499  \n","Epoch: [17][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.2003(0.9469) Grad: 0.2487  \n","EVAL: [0/16] Data 0.654 (0.654) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9246(0.9246) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.9469  avg_val_loss: 1.0132  time: 24s\n","Epoch 17 - Score: 1.0075\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.068) Elapsed 0m 2s (remain 0m 0s) Loss: 1.3362(1.0132) \n","Epoch: [18][0/92] Data 0.630 (0.630) Elapsed 0m 0s (remain 1m 30s) Loss: 1.6190(1.6190) Grad: 4.7990  \n","Epoch: [18][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0664(0.9435) Grad: 0.1074  \n","EVAL: [0/16] Data 0.715 (0.715) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8619(0.8619) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.9435  avg_val_loss: 0.9900  time: 24s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.084) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2344(0.9900) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 18 - Score: 0.9963\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [19][0/92] Data 0.479 (0.479) Elapsed 0m 0s (remain 1m 14s) Loss: 0.9256(0.9256) Grad: 0.4182  \n","Epoch: [19][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.9663(0.9471) Grad: 1.0334  \n","EVAL: [0/16] Data 0.729 (0.729) Elapsed 0m 0s (remain 0m 12s) Loss: 1.0045(1.0045) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.9471  avg_val_loss: 1.1507  time: 22s\n","Epoch 19 - Score: 1.0672\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.072) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2863(1.1507) \n","Epoch: [20][0/92] Data 0.589 (0.589) Elapsed 0m 0s (remain 1m 23s) Loss: 0.8228(0.8228) Grad: 0.5923  \n","Epoch: [20][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.6337(0.9435) Grad: 2.1208  \n","EVAL: [0/16] Data 0.602 (0.602) Elapsed 0m 0s (remain 0m 10s) Loss: 0.8507(0.8507) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.9435  avg_val_loss: 0.9621  time: 27s\n","Epoch 20 - Score: 0.9853\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.064) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2879(0.9621) \n","Epoch: [21][0/92] Data 0.532 (0.532) Elapsed 0m 0s (remain 1m 16s) Loss: 0.9145(0.9145) Grad: 1.2675  \n","Epoch: [21][91/92] Data 0.000 (0.006) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7557(0.9456) Grad: 3.0414  \n","EVAL: [0/16] Data 0.745 (0.745) Elapsed 0m 0s (remain 0m 12s) Loss: 0.8736(0.8736) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.9456  avg_val_loss: 0.9758  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.082) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2791(0.9758) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 21 - Score: 0.9924\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [22][0/92] Data 0.618 (0.618) Elapsed 0m 0s (remain 1m 30s) Loss: 1.0179(1.0179) Grad: 4.0838  \n","Epoch: [22][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0989(0.9451) Grad: 4.7238  \n","EVAL: [0/16] Data 0.705 (0.705) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8271(0.8271) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.9451  avg_val_loss: 0.9760  time: 22s\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2861(0.9760) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 22 - Score: 0.9872\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [23][0/92] Data 0.554 (0.554) Elapsed 0m 0s (remain 1m 29s) Loss: 1.0765(1.0765) Grad: 2.9631  \n","Epoch: [23][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 1.1116(0.9432) Grad: 1.6690  \n","EVAL: [0/16] Data 0.718 (0.718) Elapsed 0m 0s (remain 0m 12s) Loss: 0.9465(0.9465) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.9432  avg_val_loss: 1.0224  time: 23s\n","Epoch 23 - Score: 1.0116\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.076) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2872(1.0224) \n","Epoch: [24][0/92] Data 0.632 (0.632) Elapsed 0m 0s (remain 1m 21s) Loss: 0.8440(0.8440) Grad: 2.6218  \n","Epoch: [24][91/92] Data 0.000 (0.008) Elapsed 0m 19s (remain 0m 0s) Loss: 1.0569(0.9426) Grad: 0.9248  \n","EVAL: [0/16] Data 0.686 (0.686) Elapsed 0m 0s (remain 0m 11s) Loss: 0.9868(0.9868) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.9426  avg_val_loss: 1.0635  time: 22s\n","Epoch 24 - Score: 1.0323\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.073) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2844(1.0635) \n","Epoch: [25][0/92] Data 0.557 (0.557) Elapsed 0m 0s (remain 1m 18s) Loss: 1.1074(1.1074) Grad: 1.3699  \n","Epoch: [25][91/92] Data 0.000 (0.007) Elapsed 0m 19s (remain 0m 0s) Loss: 0.7116(0.9465) Grad: 2.4619  \n","EVAL: [0/16] Data 0.667 (0.667) Elapsed 0m 0s (remain 0m 11s) Loss: 0.8219(0.8219) \n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.9465  avg_val_loss: 0.9469  time: 28s\n","Epoch 25 - Score: 0.9744\n","========== fold: 3 result ==========\n","Score: 0.9698\n","========== CV ==========\n","Score: 0.9693\n"],"name":"stderr"},{"output_type":"stream","text":["EVAL: [15/16] Data 0.000 (0.067) Elapsed 0m 2s (remain 0m 0s) Loss: 1.2866(0.9469) \n"],"name":"stdout"},{"output_type":"stream","text":["========== Inference =========\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1608208d63bc412681ba66f593440c57","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/185 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UP_2ybJYyah0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626148581389,"user_tz":-540,"elapsed":2155,"user":{"displayName":"加藤智裕","photoUrl":"","userId":"05570068055704488630"}},"outputId":"1f0a5436-0ff0-4a5f-e5ff-d353802f1c07"},"source":["# notebookをexpの結果のdirectoryに保存\n","from requests import get\n","import subprocess\n","filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n","subprocess.run(f\"cp /content/drive/MyDrive/atmacup/atmacup11/notebook/{filename} {OUTPUT_DIR}\".split())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['cp', '/content/drive/MyDrive/atmacup/atmacup11/notebook/atmacup11_baseline1.ipynb', '/content/drive/MyDrive/atmacup/atmacup11/exp/exp005/'], returncode=0)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"D3Mz-H9h2WZ4"},"source":[""],"execution_count":null,"outputs":[]}]}